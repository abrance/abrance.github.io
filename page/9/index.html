
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Xiaoy</title>
    <meta name="author" content="xiaoy" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>XIAOY</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;XIAOY</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div
        id="home-background"
        ref="homeBackground"
        data-images="/images/background.jpg"
    ></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>Xiaoy</h1>
                <h3>xiaoy blog</h3>
                <h5></h5>
            </div>
        </span>
    </div>
</div>
<div
    id="home-posts-wrap"
    ref="homePostsWrap"
    true
>
    <div id="home-posts">
        

<div class="post">
    <a href="/2024/05/11/mdstorage/domain/golang/golang%E9%AB%98%E6%80%A7%E8%83%BD%E7%BC%96%E7%A8%8B/">
        <h2 class="post-title">golang高性能编程.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/11
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="切片语法-buf"><a href="#切片语法-buf" class="headerlink" title="切片语法 buf[:]"></a>切片语法 buf[:]</h3><p>在Go语言中，当处理数组或切片时，buf和buf[:]有不同含义：<br>buf：代表整个数组本身。数组是值类型，因此传递数组时会复制整个数组的内容，这在大多数情况下不是高效或所需的行为，特别是对于大的数据结构或I&#x2F;O操作。<br>buf[:]：创建了数组buf的一个切片视图。切片是引用类型，这意味着它不复制底层数组的数据，而是持有对原数组的引用以及一些描述切片范围的信息（起始位置和长度）。在进行读写等操作时，使用切片可以更高效地直接操作原始数据，而不需要复制整个数组。<br>在I&#x2F;O操作如unix.Read中，通常希望直接操作数据缓冲区，而不是复制它，因此使用buf[:]来传递一个指向数组的切片，这样可以避免不必要的数据复制，提高效率，并且允许函数修改缓冲区的内容。此外，这种方式让代码更加灵活，即使缓冲区的大小在未来需要调整，只需要改变数组声明，而不必改动读写操作的代码。</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/golang/" style="color: #ffa2c4">golang</a>
        </span>
        
    </div>
    <a href="/2024/05/11/mdstorage/domain/golang/golang%E9%AB%98%E6%80%A7%E8%83%BD%E7%BC%96%E7%A8%8B/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/11/mdstorage/domain/network/%E5%8D%8F%E8%AE%AE/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF/">
        <h2 class="post-title">grpc-go客户端.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/11
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="1-客户端代码示例"><a href="#1-客户端代码示例" class="headerlink" title="1 客户端代码示例"></a>1 客户端代码示例</h2><h2 id="1-1-main-函数"><a href="#1-1-main-函数" class="headerlink" title="1.1 main 函数"></a>1.1 main 函数</h2><p><img src="https://pic2.zhimg.com/v2-0fbd37c1514195654ea09974927530a1_b.jpg"></p>
<p>首先给出 grpc-go 启动客户端的代码示例，核心内容分三块：</p>
<ul>
<li><p>调用 grpc.Dial 方法，指定服务端 target，创建 grpc 连接代理对象 ClientConn</p>
</li>
<li><p>调用 proto.NewHelloServiceClient 方法，基于 pb 桩代码构造客户端实例</p>
</li>
<li><p>调用 client.SayHello 方法，真正发起 grpc 请求</p>
<p>  package main</p>
<p>  import (<br>  “context”<br>  “fmt”<br><br>  “github.com&#x2F;grpc_demo&#x2F;proto”<br><br>  “google.golang.org&#x2F;grpc”<br>  “google.golang.org&#x2F;grpc&#x2F;credentials&#x2F;insecure”<br>  )<br>  func main() {<br>  conn, err :&#x3D; grpc.Dial(“localhost:8093”, grpc.WithTransportCredentials(insecure.NewCredentials()))<br>  &#x2F;&#x2F; …<br>  defer conn.Close()<br><br>  client :&#x3D; proto.NewHelloServiceClient(conn)<br><br><br>  resp, err :&#x3D; client.SayHello(context.Background(), &amp;proto.HelloReq{<br>      Name: “xiaoxuxiansheng”,<br>  })<br>  &#x2F;&#x2F; …<br><br><br>  fmt.Printf(“resp: %+v”, resp)<br>  }</p>
</li>
</ul>
<h2 id="1-2-proto-文件"><a href="#1-2-proto-文件" class="headerlink" title="1.2 proto 文件"></a>1.2 proto 文件</h2><p>对应的 proto 文件定义：</p>
<pre><code>syntax = &quot;proto3&quot;; // 固定语法前缀
option go_package = &quot;.&quot;;  // 指定生成的Go代码在你项目中的导入路径
package pb; // 包名


// 定义服务
service HelloService &#123;
    // SayHello 方法
    rpc SayHello (HelloReq) returns (HelloResp) &#123;&#125;
&#125;


// 请求消息
message HelloReq &#123;
    string name = 1;
&#125;


// 响应消息
message HelloResp &#123;
    string reply = 1;
&#125;
</code></pre>
<h2 id="2-核心数据结构"><a href="#2-核心数据结构" class="headerlink" title="2 核心数据结构"></a>2 核心数据结构</h2><p><img src="https://pic2.zhimg.com/v2-c842a0159d6bcee06dcf5772a3443acd_b.jpg"></p>
<p>下面对 grpc-go 客户端涉及到的几个核心数据结构以及其间的关联做个介绍.</p>
<h2 id="2-1-ClientConn"><a href="#2-1-ClientConn" class="headerlink" title="2.1 ClientConn"></a>2.1 ClientConn</h2><p>ClientConn 是广义上的 grpc 连接代理对象，和 grpc 客户端是一对一的关系，内部包含了一个连接池，根据配置可能同时管理多笔连接.<br>其中几个核心字段包括：</p>
<ul>
<li><p>target&#x2F;parsedTarget：对服务端地址信息的封装</p>
</li>
<li><p>balancerWrapper：负载均衡器. 初始化时会启动一个守护协程，动态地对 ClientConn 及 Subconn 的状态进行刷新</p>
</li>
<li><p>blockingpicker：连接选择器. 在发送请求时，由其最终挑选出使用的 Subconn</p>
</li>
<li><p>resolverWrapper：解析器. 负责根据不同的 schema，通过 target 解析出服务端的实际地址</p>
<p>  type ClientConn struct {<br>  &#x2F;&#x2F; 连接上下文<br>  ctx    context.Context<br>  &#x2F;&#x2F; 上下文终止控制器<br>  cancel context.CancelFunc<br><br><br>  &#x2F;&#x2F; 连接的目标地址<br>  target          string<br>  &#x2F;&#x2F; 对连接目标地址的封装<br>  parsedTarget    resolver.Target<br>  &#x2F;&#x2F; …<br>  &#x2F;&#x2F; 连接配置项<br>  dopts           dialOptions<br>  &#x2F;&#x2F; 负载均衡器，底层基于 gracefulswitch.balancer<br>  balancerWrapper *ccBalancerWrapper<br><br><br>  &#x2F;&#x2F; 连接状态管理器<br>  csMgr              *connectivityStateManager<br>  &#x2F;&#x2F; 连接选择器<br>  blockingpicker     *pickerWrapper<br>  &#x2F;&#x2F; …<br>  &#x2F;&#x2F; 读写互斥锁<br>  mu              sync.RWMutex<br>  &#x2F;&#x2F; 解析器<br>  resolverWrapper *ccResolverWrapper<br>  &#x2F;&#x2F; 连接池<br>  conns           map[*addrConn]struct{}     &#x2F;&#x2F; Set to nil on close.<br>  &#x2F;&#x2F; …<br>  }</p>
</li>
</ul>
<h2 id="2-2-ccBalancerWrapper"><a href="#2-2-ccBalancerWrapper" class="headerlink" title="2.2 ccBalancerWrapper"></a>2.2 ccBalancerWrapper</h2><p>ccBalancerWrapper是在负载均衡器Balancer的基础上做的封装.<br>在ccBalancerWrapper被初始化时，会开启一个守护协程，通过监听 updateCh 中到达的事件，对 ClientConn 和 Subconn<br>的状态进行刷新.</p>
<pre><code>type ccBalancerWrapper struct &#123;
    // 指向所属的 clientConn
    cc *ClientConn


    // 负载均衡器
    balancer        *gracefulswitch.Balancer
    curBalancerName string
  
    // 接收更新事件的 chan
    updateCh *buffer.Unbounded 
    // 接收处理结果的 chan
    resultCh *buffer.Unbounded 
    // ...
&#125;
</code></pre>
<p>ccBalancerWrapper 的核心是一个负载均衡器 Balancer 接口,其中包含了几个核心方法:</p>
<ul>
<li><p>UpdateClientConnState：更新 ClientConn 的连接状态</p>
</li>
<li><p>ResolverError：错误后处理</p>
</li>
<li><p>UpdateSubConnState：更新子连接 Subconn 状态</p>
</li>
<li><p>Close：关闭负载均衡器</p>
<p>  type Balancer interface {<br>  UpdateClientConnState(ClientConnState) error<br>  ResolverError(error)<br>  UpdateSubConnState(SubConn, SubConnState)<br>  Close()<br>  }</p>
</li>
</ul>
<p>在默认情况下，grpc客户端框架会为我们提供一个默认的负载均衡器 pickfirstBalancer：</p>
<pre><code>type pickfirstBalancer struct &#123;
    state   connectivity.State
    cc      balancer.ClientConn
    subConn balancer.SubConn
&#125;
</code></pre>
<p>与pickfirstBalancer具体的交互流程我们在第3章再作展开.</p>
<h2 id="2-3-ccResolverWrapper"><a href="#2-3-ccResolverWrapper" class="headerlink" title="2.3 ccResolverWrapper"></a>2.3 ccResolverWrapper</h2><p>ccResolverWrapper的核心是内置的解析器 resolver.</p>
<pre><code>type ccResolverWrapper struct &#123;
    // 指向所属的 clientConn
    cc         *ClientConn
    resolverMu sync.Mutex
    // 核心成员：内置的解析器
    resolver   resolver.Resolver
    // ...
&#125;
</code></pre>
<p>resolver通过Builder构造，对应的Buidler是一个interface,用户也可以提供自己的实现版本:</p>
<pre><code>type Builder interface &#123;
    // 构造解析器 resolver
    Build(target Target, cc ClientConn, opts BuildOptions) (Resolver, error)
    // Scheme returns the scheme supported by this resolver.
    // Scheme is defined at https://github.com/grpc/grpc/blob/master/doc/naming.md.
    Scheme() string
&#125;
</code></pre>
<p>resolver本身是一个接口，核心的方法是 ResolveNow：通过 target 解析出实际的客户端地址.</p>
<pre><code>type Resolver interface &#123;
    // 解析 target
    ResolveNow(ResolveNowOptions)
    // 关闭 resolver
    Close()
&#125;
</code></pre>
<p>grpc客户端为我们提供了一个默认的resolver：passthroughResolver：</p>
<pre><code>type passthroughResolver struct &#123;
    target resolver.Target
    cc     resolver.ClientConn
&#125;
</code></pre>
<p>passthroughResolver在解析 target 时的策略是直接透传不作处理. 具体交互流程见第3章.</p>
<h2 id="2-4-ClientStream"><a href="#2-4-ClientStream" class="headerlink" title="2.4 ClientStream"></a>2.4 ClientStream</h2><p>在grpc客户端客户端发起请求时，会首先创建出一个 ClientStream，并依赖其核心方法 SendMsg 和 RecvMsg<br>进行请求的发送和响应的接受.</p>
<pre><code>type ClientStream interface &#123;
    // 获取元数据
    Header() (metadata.MD, error)
    // 获取上下文
    Context() context.Context
    // 发送消息
    SendMsg(m interface&#123;&#125;) error
    // 接收消息
    RecvMsg(m interface&#123;&#125;) error
&#125;
</code></pre>
<p>ClientStream是一个interface，其默认的实现是 clientStream：</p>
<pre><code>type clientStream struct &#123;
    // ...
    cc       *ClientConn
    desc     *StreamDesc


    // 编码模块
    codec baseCodec
    // 压缩模块
    cp    Compressor
    comp  encoding.Compressor
    // ...
    // 上下文
    ctx context.Context 
    // 请求尝试
    attempt *csAttempt
    // ...
&#125;
</code></pre>
<h2 id="2-5-csAttempt"><a href="#2-5-csAttempt" class="headerlink" title="2.5 csAttempt"></a>2.5 csAttempt</h2><p>csAttempt 代表了一次 grpc 请求尝试，本身是具有生命周期的.</p>
<pre><code>type csAttempt struct &#123;
    ctx        context.Context
    cs         *clientStream
    t          transport.ClientTransport
    s          *transport.Stream
    p          *parser
    pickResult balancer.PickResult


    // 解压模块
    dc        Decompressor
    decomp    encoding.Compressor
    decompSet bool


    // ...
&#125;
</code></pre>
<h2 id="3-grpc-Dial"><a href="#3-grpc-Dial" class="headerlink" title="3 grpc.Dial"></a>3 grpc.Dial</h2><p><img src="https://pic2.zhimg.com/v2-d55c7ce48f8ab26c9f0d712e5bb75125_b.jpg"></p>
<h2 id="3-1-grpc-Dial"><a href="#3-1-grpc-Dial" class="headerlink" title="3.1 grpc.Dial"></a>3.1 grpc.Dial</h2><pre><code>func Dial(target string, opts ...DialOption) (*ClientConn, error) &#123;
    return DialContext(context.Background(), target, opts...)
&#125;
func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) &#123;
    cc := &amp;ClientConn&#123;
        target:            target,
        csMgr:             &amp;connectivityStateManager&#123;&#125;,
        conns:             make(map[*addrConn]struct&#123;&#125;),
        dopts:             defaultDialOptions(),
        blockingpicker:    newPickerWrapper(),
        czData:            new(channelzData),
        firstResolveEvent: grpcsync.NewEvent(),
    &#125;
    // ...
    // Determine the resolver to use.
    resolverBuilder, err := cc.parseTargetAndFindResolver()
    // ...
    cc.balancerWrapper = newCCBalancerWrapper(cc, balancer.BuildOptions&#123;
        DialCreds:        credsClone,
        CredsBundle:      cc.dopts.copts.CredsBundle,
        Dialer:           cc.dopts.copts.Dialer,
        Authority:        cc.authority,
        CustomUserAgent:  cc.dopts.copts.UserAgent,
        ChannelzParentID: cc.channelzID,
        Target:           cc.parsedTarget,
    &#125;)
    // ...
    rWrapper, err := newCCResolverWrapper(cc, resolverBuilder)
    // ...
    cc.mu.Lock()
    cc.resolverWrapper = rWrapper
    cc.mu.Unlock()
    // ...
    return cc, nil
&#125;
</code></pre>
<p>在通过 DialContext 创建 grpc 连接代理 ClientConn 时，核心步骤包括：</p>
<ul>
<li>创建 ClientConn 实例</li>
<li>调用 ClientConn.parseTargetAndFindResolver 方法，通过 target 中的 schema 获取到对应的解析器构造器 resolverBuilder</li>
<li>调用 newCCBalancerWrapper 方法构造出负载均衡器封装对象 ccBalancerWrapper，在内部会开启守护协程感知和处理 ClientConn 和 Subconn 状态变更的事件</li>
<li>调用 newCCResolverWrapper 方法，内部会调用 resolverBuilder 构造并启动 resolver 实例，同时会通过 ccBalancerWrapper 方法对 ClientConn 的状态进行更新</li>
</ul>
<h2 id="3-2-ClientConn-parseTargetAndFindResolver"><a href="#3-2-ClientConn-parseTargetAndFindResolver" class="headerlink" title="3.2 ClientConn.parseTargetAndFindResolver"></a>3.2 ClientConn.parseTargetAndFindResolver</h2><p><img src="https://pic1.zhimg.com/v2-2d9d5ced9a49aaeba25003e2b27a4f88_b.jpg"></p>
<pre><code>func (cc *ClientConn) parseTargetAndFindResolver() (resolver.Builder, error) &#123;
    // ...
    var rb resolver.Builder
    parsedTarget, err := parseTarget(cc.target)
    
    rb = cc.getResolver(parsedTarget.URL.Scheme)
    if rb != nil &#123;
        cc.parsedTarget = parsedTarget
        return rb, nil
    &#125;
    // ...
&#125;
</code></pre>
<p>ClientConn.parseTargetAndFindResolver 方法通过 target 中的 schema，会获取到对应的<br>resolverBuilder，后续用于构建出能解析出服务端地址的 resolver.</p>
<p>在 grpc-go 的 resolver 包下，会通过一个全局 map 实现 schema 到 resolverBuilder 的映射，同时会暴露出注册方法<br>Register，供用户自定义实现特定 schema 下的 resolverBuilder 和 resolver 并注入到 map 中.</p>
<pre><code>var (
    m = make(map[string]Builder)
    defaultScheme = &quot;passthrough&quot;
)


func Register(b Builder) &#123;
    m[b.Scheme()] = b
&#125;


func Get(scheme string) Builder &#123;
    if b, ok := m[scheme]; ok &#123;
        return b
    &#125;
    return nil
&#125;
</code></pre>
<p>grpc-go 中，默认的 resolverBuilder 和对应的 resolver 是 passthrough 类型，这类 resolver<br>的解析策略是对 target 直接透传，不作解析处理.</p>
<p>其中，passthroughBuilder.Build 方法中，会执行 passthroughResolver.start<br>方法一键启动解析器，这部分逻辑我们放到 3.4 小节中，在 passthroughBuilder.Build 方法真正被调用时再作展开.</p>
<pre><code>const scheme = &quot;passthrough&quot;


type passthroughBuilder struct&#123;&#125;


func (*passthroughBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) &#123;
    // ...
    r := &amp;passthroughResolver&#123;
        target: target,
        cc:     cc,
    &#125;
    r.start()
    return r, nil
&#125;


type passthroughResolver struct &#123;
    target resolver.Target
    cc     resolver.ClientConn
&#125;


func (r *passthroughResolver) start() &#123;
    r.cc.UpdateState(resolver.State&#123;Addresses: []resolver.Address&#123;&#123;Addr: r.target.Endpoint()&#125;&#125;&#125;)
&#125;


func (*passthroughResolver) ResolveNow(o resolver.ResolveNowOptions)&#123;&#125;
</code></pre>
<h2 id="3-3-newCCBalancerWrapper"><a href="#3-3-newCCBalancerWrapper" class="headerlink" title="3.3 newCCBalancerWrapper"></a>3.3 newCCBalancerWrapper</h2><pre><code>func newCCBalancerWrapper(cc *ClientConn, bopts balancer.BuildOptions) *ccBalancerWrapper &#123;
    ccb := &amp;ccBalancerWrapper&#123;
        cc:       cc,
        updateCh: buffer.NewUnbounded(),
        resultCh: buffer.NewUnbounded(),
        closed:   grpcsync.NewEvent(),
        done:     grpcsync.NewEvent(),
    &#125;
    go ccb.watcher()
    ccb.balancer = gracefulswitch.NewBalancer(ccb, bopts)
    return ccb
&#125;
</code></pre>
<p><img src="https://pic3.zhimg.com/v2-3a59f74b043f0c9e9494a4738390eb06_b.jpg"></p>
<pre><code>func (ccb *ccBalancerWrapper) watcher() &#123;
    for &#123;
        select &#123;
        case u := &lt;-ccb.updateCh.Get():
            ccb.updateCh.Load()
            if ccb.closed.HasFired() &#123;
                break
            &#125;
            switch update := u.(type) &#123;
            case *ccStateUpdate:
                ccb.handleClientConnStateChange(update.ccs)
            case *scStateUpdate:
                ccb.handleSubConnStateChange(update)
            case *exitIdleUpdate:
                ccb.handleExitIdle()
            case *resolverErrorUpdate:
                ccb.handleResolverError(update.err)
            case *switchToUpdate:
                ccb.handleSwitchTo(update.name)
            case *subConnUpdate:
                ccb.handleRemoveSubConn(update.acbw)
            default:
                logger.Errorf(&quot;ccBalancerWrapper.watcher: unknown update %+v, type %T&quot;, update, update)
            &#125;
        case &lt;-ccb.closed.Done():
        &#125;


        // ...
    &#125;
&#125;
</code></pre>
<p>newCCBalancerWrapper 方法构造了 ccBalancer 实例，然后调用 ccBalancerWrapper.watcher<br>方法开启守护协程，分别监听 ClientConn 状态变更（ccStateUpdate）、Subconn<br>状态变更（scStateUpdate）、设定负载均衡器<br>balancer（switchToUpdate）、连接移除（subConnUpdate）等事件，并分别进行处理.</p>
<p>在 newCCBalancerWrapper 方法中，还调用了 gracefulswitch.NewBalancer<br>构造了内置负载均衡器的外壳，但真正的负载均衡器 Balancer 此时还未注入，注入实际会在 3.4 小节，resovler 启动的链路当中.</p>
<h2 id="3-4-newCCResolverWrapper"><a href="#3-4-newCCResolverWrapper" class="headerlink" title="3.4 newCCResolverWrapper"></a>3.4 newCCResolverWrapper</h2><p><img src="https://pic3.zhimg.com/v2-0d381d5a8047a3677b7065974dbf43fa_b.png"></p>
<pre><code>func newCCResolverWrapper(cc *ClientConn, rb resolver.Builder) (*ccResolverWrapper, error) &#123;
    ccr := &amp;ccResolverWrapper&#123;
        cc:   cc,
        done: grpcsync.NewEvent(),
    &#125;


    // ...


    var err error
    ccr.resolverMu.Lock()
    defer ccr.resolverMu.Unlock()
    ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, rbo)
    // ...
    return ccr, nil
&#125;
</code></pre>
<p>newCCResolverWrapper 方法构造了 ccResolverWrapper 实例，但真正的核心逻辑是根据传入的 resolverBuilder<br>构造器出对应的 resolver 然后注入到 ccResolverWrapper 当中.</p>
<p>grpc-go 客户端默认的 resovlerBuilder 和 resolver 是 passthrough 类型，下面我们再一次展开</p>
<p>passthroughBuilder.Build 方法来看：</p>
<pre><code>func (*passthroughBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) &#123;
    // ...
    r := &amp;passthroughResolver&#123;
        target: target,
        cc:     cc,
    &#125;
    r.start()
    return r, nil
&#125;
</code></pre>
<p>在方法中，构造了一个 passthroughResolver 实例，并在返回前调用了 passthroughResolver.start 方法启动了解析器.</p>
<pre><code>func (r *passthroughResolver) start() &#123;
    r.cc.UpdateState(resolver.State&#123;Addresses: []resolver.Address&#123;&#123;Addr: r.target.Endpoint()&#125;&#125;&#125;)
&#125;
</code></pre>
<p><img src="https://pic3.zhimg.com/v2-89bca7305f47501e10a753fdff912812_b.png"></p>
<p>此处直接透传了用户传入的服务端地址 target.Endpoint，经历了 ccResolverWrapper.UpdateState -&gt;<br>ClientConn.updateResolverState 的调用链路，然后分别通过 ccBalancerWrapper.switchTo 和<br>ccBalancerWrapper.updateClientConnState 方法，通过 updateCh 向 ccBalancerWrapper<br>传递了设定 balancer 和更新 ClientConn 的事件.</p>
<pre><code>func (ccr *ccResolverWrapper) UpdateState(s resolver.State) error &#123;
    // ...
    ccr.curState = s
    if err := ccr.cc.updateResolverState(ccr.curState, nil); err == balancer.ErrBadResolverState &#123;
        return balancer.ErrBadResolverState
    &#125;
    return nil
&#125;
func (cc *ClientConn) updateResolverState(s resolver.State, err error) error &#123;
    // ...
    cc.maybeApplyDefaultServiceConfig(s.Addresses)
    // ...    
    uccsErr := bw.updateClientConnState(&amp;balancer.ClientConnState&#123;ResolverState: s, BalancerConfig: balCfg&#125;)
    // ...
&#125;
</code></pre>
<h3 id="3-4-1-ccBalancerWrapper-switchTo"><a href="#3-4-1-ccBalancerWrapper-switchTo" class="headerlink" title="3.4.1 ccBalancerWrapper.switchTo"></a><strong>3.4.1 ccBalancerWrapper.switchTo</strong></h3><pre><code>func (cc *ClientConn) maybeApplyDefaultServiceConfig(addrs []resolver.Address) &#123;
    // ...
    cc.applyServiceConfigAndBalancer(emptyServiceConfig, &amp;defaultConfigSelector&#123;emptyServiceConfig&#125;, addrs)
&#125;
const PickFirstBalancerName = &quot;pick_first&quot;


func (cc *ClientConn) applyServiceConfigAndBalancer(sc *ServiceConfig, configSelector iresolver.ConfigSelector, addrs []resolver.Address) &#123;
    // 倘若未通过 config 设定，则默认的 balancer 是 pickFirst
    newBalancerName = PickFirstBalancerName
    // ...
    cc.balancerWrapper.switchTo(newBalancerName)
&#125;
func (ccb *ccBalancerWrapper) switchTo(name string) &#123;
    ccb.updateCh.Put(&amp;switchToUpdate&#123;name: name&#125;)
&#125;
</code></pre>
<p><img src="https://pic2.zhimg.com/v2-c485ed2672bd1d62f47013e2daad7861_b.png"></p>
<p>接下来在 ccBalancerWrapper 的守护协程中，一旦接收到 swtichToUpdate 类型的消息后，会执行<br>ccBalancerWrapper.handleSwitchTo 执行负载均衡器 Balancer 的设定.</p>
<pre><code>func (ccb *ccBalancerWrapper) watcher() &#123;
    for &#123;
        select &#123;
        case u := &lt;-ccb.updateCh.Get():
            ccb.updateCh.Load()
            // ...
            switch update := u.(type) &#123;
            // ...
            case *switchToUpdate:
                ccb.handleSwitchTo(update.name)
            // ...
            &#125;
        // ...
        &#125;
        // ...
    &#125;
&#125;
func (ccb *ccBalancerWrapper) handleSwitchTo(name string) &#123;
    // ...
    builder := balancer.Get(name)
    if builder == nil &#123;
        // ...
        builder = newPickfirstBuilder()
    &#125; 
    if err := ccb.balancer.SwitchTo(builder); err != nil &#123;
        // ...
    &#125;
    ccb.curBalancerName = builder.Name()
&#125;
</code></pre>
<p>在 ccBalancerWrapper.handleSwitchTo 方法中，由于传入的 name 为 pick_first，因此会构造出对应的<br>pickerFirst balancer 并注入到 ccBalancerWrapper 中.</p>
<pre><code>func (gsb *Balancer) SwitchTo(builder balancer.Builder) error &#123;
    // ...
    bw := &amp;balancerWrapper&#123;
        gsb: gsb,
        lastState: balancer.State&#123;
            ConnectivityState: connectivity.Connecting,
            Picker:            base.NewErrPicker(balancer.ErrNoSubConnAvailable),
        &#125;,
        subconns: make(map[balancer.SubConn]bool),
    &#125;
    // ...
    if gsb.balancerCurrent == nil &#123;
        gsb.balancerCurrent = bw
    &#125; else &#123;
        gsb.balancerPending = bw
    &#125;
    // ...
    newBalancer := builder.Build(bw, gsb.bOpts)
    // ...
    // ...
    bw.Balancer = newBalancer
    return nil
&#125;
</code></pre>
<p>其中 pickerFirst balancer 定义如下：</p>
<pre><code>const PickFirstBalancerName = &quot;pick_first&quot;


type pickfirstBuilder struct&#123;&#125;


func (*pickfirstBuilder) Build(cc balancer.ClientConn, opt balancer.BuildOptions) balancer.Balancer &#123;
    return &amp;pickfirstBalancer&#123;cc: cc&#125;
&#125;


func (*pickfirstBuilder) Name() string &#123;
    return PickFirstBalancerName
&#125;


type pickfirstBalancer struct &#123;
    state   connectivity.State
    cc      balancer.ClientConn
    subConn balancer.SubConn
&#125;
</code></pre>
<h3 id="3-4-2-ccBalancerWrapper-updateClientConnState"><a href="#3-4-2-ccBalancerWrapper-updateClientConnState" class="headerlink" title="3.4.2 ccBalancerWrapper.updateClientConnState"></a><strong>3.4.2 ccBalancerWrapper.updateClientConnState</strong></h3><p><img src="https://pic3.zhimg.com/v2-9d474750a20378dbc9a8724c4f20720e_b.jpg"></p>
<p>重新回到 ClientConn.updateResolverState 方法中，在处理完 ccBalancerWrapper.switchTo<br>分支完成负载均衡器 Balancer 的创建和设置后，会调用ccBalancerWrapper.updateClientConnState 方法对<br>ClientConn 的状态进行更新.</p>
<pre><code>func (ccb *ccBalancerWrapper) updateClientConnState(ccs *balancer.ClientConnState) error &#123;
    ccb.updateCh.Put(&amp;ccStateUpdate&#123;ccs: ccs&#125;)
    // ...
&#125;
</code></pre>
<p>ccBalancerWrapper 的守护协程接收到 ccStateUpdate 消息后，会调用<br>ccBalancerWrapper.handleClientConnStateChange 方法对 ClientConn 的状态进行更新.</p>
<pre><code>func (ccb *ccBalancerWrapper) watcher() &#123;
    for &#123;
        select &#123;
        case u := &lt;-ccb.updateCh.Get():
            ccb.updateCh.Load()
            // ...
            switch update := u.(type) &#123;
            case *ccStateUpdate:
                ccb.handleClientConnStateChange(update.ccs)
            // ...
            &#125;
        // ...
        &#125;
        // ...
    &#125;
&#125;
func (ccb *ccBalancerWrapper) handleClientConnStateChange(ccs *balancer.ClientConnState) &#123;
    // ...
    ccb.resultCh.Put(ccb.balancer.UpdateClientConnState(*ccs))
&#125;
func (gsb *Balancer) UpdateClientConnState(state balancer.ClientConnState) error &#123;
    // ...
    return balToUpdate.UpdateClientConnState(state)
&#125;
</code></pre>
<p>此处会一路走到负载均衡器 Balancer 的 UpdateClientConnState 方法中. 对应的 Balancer 为 pickFirst<br>类型，我们展开对应的方法源码：</p>
<pre><code>func (b *pickfirstBalancer) UpdateClientConnState(state balancer.ClientConnState) error &#123;
    // ...
    subConn, err := b.cc.NewSubConn(state.ResolverState.Addresses, balancer.NewSubConnOptions&#123;&#125;)
    // ...
    b.subConn = subConn
    b.state = connectivity.Idle
    b.cc.UpdateState(balancer.State&#123;
        ConnectivityState: connectivity.Connecting,
        Picker:            &amp;picker&#123;err: balancer.ErrNoSubConnAvailable&#125;,
    &#125;)
    b.subConn.Connect()
    return nil
&#125;
</code></pre>
<p>pickfirstBalancer.UpdateClientConnState 方法的核心步骤包括：</p>
<ul>
<li>调用 ccBalancerWrapper.NewSubConn 方法创建了子连接 subconn</li>
<li>调用了 ccBalancerWrapper.UpdateState 方法，</li>
<li>调用了 subconn.Connect 方法，建立子连接</li>
</ul>
<p><strong>（1）ccBalancerWrapper.NewSubConn</strong></p>
<pre><code>func (ccb *ccBalancerWrapper) NewSubConn(addrs []resolver.Address, opts balancer.NewSubConnOptions) (balancer.SubConn, error) &#123;
    // ...
    ac, err := ccb.cc.newAddrConn(addrs, opts)
    // ...
    acbw := &amp;acBalancerWrapper&#123;ac: ac, producers: make(map[balancer.ProducerBuilder]*refCountedProducer)&#125;
    acbw.ac.mu.Lock()
    ac.acbw = acbw
    acbw.ac.mu.Unlock()
    return acbw, nil
&#125;
</code></pre>
<p>在 ccBalancerWrapper.NewSubConn 方法中，基于 target 的 address 构造了连接 addrConn，并将其封装到<br>balancer.SubConn 具体实现类 acBalancerWrapper 中进行返回.</p>
<pre><code>type acBalancerWrapper struct &#123;
    mu        sync.Mutex
    ac        *addrConn
    // ...
&#125;
type addrConn struct &#123;
    ctx    context.Context
    cancel context.CancelFunc


    cc     *ClientConn
    dopts  dialOptions
    acbw   balancer.SubConn
    // ...
    transport transport.ClientTransport // The current transport.


    mu      sync.Mutex
    curAddr resolver.Address  
    addrs   []resolver.Address 
    state connectivity.State


    // ...
&#125;
func (cc *ClientConn) newAddrConn(addrs []resolver.Address, opts balancer.NewSubConnOptions) (*addrConn, error) &#123;
    ac := &amp;addrConn&#123;
        state:        connectivity.Idle,
        cc:           cc,
        addrs:        addrs,
        scopts:       opts,
        dopts:        cc.dopts,
        czData:       new(channelzData),
        resetBackoff: make(chan struct&#123;&#125;),
    &#125;
    // ...
    cc.conns[ac] = struct&#123;&#125;&#123;&#125;
    return ac, nil
&#125;
</code></pre>
<p>在构造 addrConn 实例的过程中会将其添加到 ClientConn 的连接池集合 ClientConn.conns 当中.</p>
<p><strong>（2）ccBalancerWrapper.UpdateState</strong></p>
<p>在 ccBalancerWrapper.UpdateState 方法中，会对连接选择器 picker 进行设定，然后调用<br>connectivityStateManager.updateState 方法将连接状态设定为 connecting 连接中.</p>
<pre><code>func (ccb *ccBalancerWrapper) UpdateState(s balancer.State) &#123;
    // ...
    ccb.cc.blockingpicker.updatePicker(s.Picker)
    ccb.cc.csMgr.updateState(s.ConnectivityState)
&#125;
func (pw *pickerWrapper) updatePicker(p balancer.Picker) &#123;
    pw.mu.Lock()
    if pw.done &#123;
        pw.mu.Unlock()
        return
    &#125;
    pw.picker = p
    // pw.blockingCh should never be nil.
    close(pw.blockingCh)
    pw.blockingCh = make(chan struct&#123;&#125;)
    pw.mu.Unlock()
&#125;
</code></pre>
<p>此处设定的连接选择器 picker 的类型默认为 pickFirst picker. 其中内置了唯一一个 pickResult，在 pick<br>时会固定返回这个结果.</p>
<pre><code>type picker struct &#123;
    result balancer.PickResult
    err    error
&#125;


func (p *picker) Pick(balancer.PickInfo) (balancer.PickResult, error) &#123;
    return p.result, p.err
&#125;
</code></pre>
<p><strong>（3）pickfirstBalancer.UpdateClientConnState</strong></p>
<p><img src="https://pic4.zhimg.com/v2-ed61d7eba4ac747d8a3077fcb5ed4f9f_b.jpg"></p>
<p>最后，pickfirstBalancer.UpdateClientConnState 方法还会调用 acBalancerWrapper.Connect<br>方法启动子连接. 其中会异步调用 addrConn.connect 方法</p>
<pre><code>func (acbw *acBalancerWrapper) Connect() &#123;
    acbw.mu.Lock()
    defer acbw.mu.Unlock()
    go acbw.ac.connect()
&#125;
func (ac *addrConn) connect() error &#123;
    ac.mu.Lock()
    // ...
    ac.updateConnectivityState(connectivity.Connecting, nil)
    // ...
    ac.resetTransport()
    return nil
&#125;
</code></pre>
<p>其中会首先通过 addrConn.updateConnectivityState 暂时先将子连接状态设置为 connecting ，后续<br>addrConn.resetTransport 方法中，经由 addrConn.tryAllAddrs -&gt;<br>addrConn.createTransport -&gt; addrConn.startHealthCheck -&gt;<br>ClientConn.handleSubConnStateChange -&gt; ccBalancerWrapper.updateSubConnState<br>的方法链路，会向 ccBalancerWrapper.updateCh 中发送一条 scStateUpdate 类型的消息，将子连接 subconn<br>状态更新为 ready 添加到 picker 的 pickerResult 当中.</p>
<pre><code>func (ac *addrConn) resetTransport() &#123;
    // ...
    addrs := ac.addrs
    // ...
    if err := ac.tryAllAddrs(addrs, connectDeadline); err != nil &#123;
        //  ...
    &#125;
    // ...
&#125;
func (ac *addrConn) tryAllAddrs(addrs []resolver.Address, connectDeadline time.Time) error &#123;
    var firstConnErr error
    for _, addr := range addrs &#123;
        // ...
        err := ac.createTransport(addr, copts, connectDeadline)
        if err == nil &#123;
            return nil
        &#125;
        // ...
    &#125;
    // ...
&#125;
func (ac *addrConn) createTransport(addr resolver.Address, copts transport.ConnectOptions, connectDeadline time.Time) error &#123;
    addr.ServerName = ac.cc.getServerName(addr)
    hctx, hcancel := context.WithCancel(ac.ctx)
    // ...


    newTr, err := transport.NewClientTransport(connectCtx, ac.cc.ctx, addr, copts, onClose)
    // ...
    ac.curAddr = addr
    ac.transport = newTr
    ac.startHealthCheck(hctx) 
    return nil
&#125;
func (ac *addrConn) startHealthCheck(ctx context.Context) &#123;
    var healthcheckManagingState bool
    defer func() &#123;
        if !healthcheckManagingState &#123;
            ac.updateConnectivityState(connectivity.Ready, nil)
        &#125;
    &#125;()
    // ...
    healthCheckFunc := ac.cc.dopts.healthCheckFunc
    if healthCheckFunc == nil &#123;
        // ...
        return
    &#125;


    // ...
&#125;
func (ac *addrConn) updateConnectivityState(s connectivity.State, lastErr error) &#123;
    // ...
    ac.state = s
    // ...
    ac.cc.handleSubConnStateChange(ac.acbw, s, lastErr)
&#125;
func (cc *ClientConn) handleSubConnStateChange(sc balancer.SubConn, s connectivity.State, err error) &#123;
    cc.balancerWrapper.updateSubConnState(sc, s, err)
&#125;
func (ccb *ccBalancerWrapper) updateSubConnState(sc balancer.SubConn, s connectivity.State, err error) &#123;
    // ...
    ccb.updateCh.Put(&amp;scStateUpdate&#123;
        sc:    sc,
        state: s,
        err:   err,
    &#125;)
&#125;
</code></pre>
<p>ccBalancerWrapper 的守护协程中，监听到 scStateUpdate 事件到达后，会经历<br>ccBalancerWrapper.handleSubConnStateChange -&gt;<br>gracefulswitch.Balancer.UpdateSubConnState -&gt;<br>pickfirstBalancer.UpdateSubConnState -&gt; ccBalancerWrapper.UpdateState<br>的方法链路，完成对 pickfirstPicker 中 pickerResult 的更新.</p>
<pre><code>func (ccb *ccBalancerWrapper) watcher() &#123;
    for &#123;
        select &#123;
        case u := &lt;-ccb.updateCh.Get():
            ccb.updateCh.Load()
            // ...
            switch update := u.(type) &#123;
            // ...
            case *scStateUpdate:
                ccb.handleSubConnStateChange(update)
            // ...
            &#125;
        case &lt;-ccb.closed.Done():
        &#125;
        // ...
    &#125;
&#125;
func (ccb *ccBalancerWrapper) handleSubConnStateChange(update *scStateUpdate) &#123;
    ccb.balancer.UpdateSubConnState(update.sc, balancer.SubConnState&#123;ConnectivityState: update.state, ConnectionError: update.err&#125;)
&#125;
func (gsb *Balancer) UpdateSubConnState(sc balancer.SubConn, state balancer.SubConnState) &#123;
    // ...
    balToUpdate.UpdateSubConnState(sc, state)
&#125;
func (b *pickfirstBalancer) UpdateSubConnState(subConn balancer.SubConn, state balancer.SubConnState) &#123;
    // ...


    switch state.ConnectivityState &#123;
    case connectivity.Ready:
        b.cc.UpdateState(balancer.State&#123;
            ConnectivityState: state.ConnectivityState,
            Picker:            &amp;picker&#123;result: balancer.PickResult&#123;SubConn: subConn&#125;&#125;,
        &#125;)
    case connectivity.Connecting:
        b.cc.UpdateState(balancer.State&#123;
            ConnectivityState: state.ConnectivityState,
            Picker:            &amp;picker&#123;err: balancer.ErrNoSubConnAvailable&#125;,
        &#125;)
    // ...
    &#125;
&#125;
func (ccb *ccBalancerWrapper) UpdateState(s balancer.State) &#123;
    // ...
    ccb.cc.blockingpicker.updatePicker(s.Picker)
    ccb.cc.csMgr.updateState(s.ConnectivityState)
&#125;
</code></pre>
<p>到这里为止，通过 grpc.Dial 方法构造 ClientConn 对象，为客户端请求进行前置准备的主线链路就已经梳理完毕.</p>
<h2 id="4-grpc-Invoke"><a href="#4-grpc-Invoke" class="headerlink" title="4 grpc.Invoke"></a>4 grpc.Invoke</h2><p><img src="https://pic1.zhimg.com/v2-e0813cacbb7221902ad1f611739f26f0_b.jpg"></p>
<h2 id="4-1-ClientConn-Invoke"><a href="#4-1-ClientConn-Invoke" class="headerlink" title="4.1 ClientConn.Invoke"></a>4.1 ClientConn.Invoke</h2><p>在grpc-go客户端通过pb桩代码发起请求时，内部会调用 ClientConn.Invoke 方法，核心步骤分为三部分：</p>
<ul>
<li><p>调用 newClientStream 方法构造一个 clientStream 用于与服务端的通信交互</p>
</li>
<li><p>调用 clientStream.SendMsg 方法发送请求</p>
</li>
<li><p>调用 clientStream.RecvMsg 方法接收响应</p>
<p>  func (c *helloServiceClient) SayHello(ctx context.Context, in *HelloReq, opts …grpc.CallOption) (*HelloResp, error) {<br>  out :&#x3D; new(HelloResp)<br>  err :&#x3D; c.cc.Invoke(ctx, “&#x2F;pb.HelloService&#x2F;SayHello”, in, out, opts…)<br>  &#x2F;&#x2F; …<br>  return out, nil<br>  }<br>  func (cc *ClientConn) Invoke(ctx context.Context, method string, args, reply interface{}, opts …CallOption) error {<br>  &#x2F;&#x2F; …<br>  return invoke(ctx, method, args, reply, cc, opts…)<br>  }<br>  func invoke(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, opts …CallOption) error {<br>  cs, err :&#x3D; newClientStream(ctx, unaryStreamDesc, cc, method, opts…)<br>  &#x2F;&#x2F; …<br>  if err :&#x3D; cs.SendMsg(req); err !&#x3D; nil {<br>      return err<br>  }<br>  return cs.RecvMsg(reply)<br>  }</p>
</li>
</ul>
<h2 id="4-2-newClientStream"><a href="#4-2-newClientStream" class="headerlink" title="4.2 newClientStream"></a>4.2 newClientStream</h2><h3 id="4-2-1-newClientStreamWithParams"><a href="#4-2-1-newClientStreamWithParams" class="headerlink" title="4.2.1 newClientStreamWithParams"></a>4.2.1 newClientStreamWithParams</h3><pre><code>func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (_ ClientStream, err error) &#123;
    // ...
    var newStream = func(ctx context.Context, done func()) (iresolver.ClientStream, error) &#123;
        return newClientStreamWithParams(ctx, desc, cc, method, mc, onCommit, done, opts...)
    &#125;
    // ...
    return newStream(ctx, func() &#123;&#125;)
&#125;
func newClientStreamWithParams(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, mc serviceconfig.MethodConfig, onCommit, doneFunc func(), opts ...CallOption) (_ iresolver.ClientStream, err error) &#123;
    // ...
    // 初始化压缩模块
    var cp Compressor
    // ...
    
    cs := &amp;clientStream&#123;
        callHdr:      callHdr,
        ctx:          ctx,
        methodConfig: &amp;mc,
        opts:         opts,
        callInfo:     c,
        cc:           cc,
        desc:         desc,
        codec:        c.codec,
        cp:           cp,
        comp:         comp,
        cancel:       cancel,
        firstAttempt: true,
        onCommit:     onCommit,
    &#125;
      
    // ...
    // 获取 csAttempt 的闭包函数
    op := func(a *csAttempt) error &#123;
        if err := a.getTransport(); err != nil &#123;
            return err
        &#125;
        if err := a.newStream(); err != nil &#123;
            return err
        &#125;
        // ...
        cs.attempt = a
        return nil
    &#125;
    if err := cs.withRetry(op, func() &#123; cs.bufferForRetryLocked(0, op) &#125;); err != nil &#123;
        return nil, err
    &#125;


    // ...
    return cs, nil
&#125;
</code></pre>
<h3 id="4-2-2-clientStream-withRetry"><a href="#4-2-2-clientStream-withRetry" class="headerlink" title="4.2.2 clientStream.withRetry"></a>4.2.2 clientStream.withRetry</h3><p>在 clientStream.withRetry 方法中，会针对 op 方法进行重试，直到处理成功或者返回的错误类型为 io.EOF.</p>
<pre><code>func (cs *clientStream) withRetry(op func(a *csAttempt) error, onSuccess func()) error &#123;
    // ...
    for &#123;
        // ...
        err := op(a)
        // ...
        if err == io.EOF &#123;
            &lt;-a.s.Done()
        &#125;
        // 处理完成
        if err == nil || (err == io.EOF &amp;&amp; a.s.Status().Code() == codes.OK) &#123;
            onSuccess()
            cs.mu.Unlock()
            return err
        &#125;
        // ...
    &#125;
&#125;
</code></pre>
<h3 id="4-2-3-csAttempt-getTransport"><a href="#4-2-3-csAttempt-getTransport" class="headerlink" title="4.2.3 csAttempt.getTransport"></a>4.2.3 csAttempt.getTransport</h3><p><img src="https://pic4.zhimg.com/v2-72d8df173eeae16f37bd03ad7e63dd2f_b.jpg"></p>
<p>在 csAttempt.getTransport 方法链路中，最终会通过 pickFirstPicker.pick 方法获取到对应的<br>pickResult，拿到用于请求的的子连接 subconn.</p>
<p>此处呼应了本文 3.4.2 小节第（3）部分，在构建 resolver 过程中，就提前准备并添加到 pickFirstPicker 中的<br>pickResult.</p>
<p>在获取到子连接 subconn 后，会调用其中的 addrConn.getReadyTranport 获取到通信器 transport.<br>这部分在后续单开的通信篇再作展开.</p>
<pre><code>func (a *csAttempt) getTransport() error &#123;
    cs := a.cs


    var err error
    a.t, a.pickResult, err = cs.cc.getTransport(a.ctx, cs.callInfo.failFast, cs.callHdr.Method)
    // ...
    return nil
&#125;
func (cc *ClientConn) getTransport(ctx context.Context, failfast bool, method string) (transport.ClientTransport, balancer.PickResult, error) &#123;
    return cc.blockingpicker.pick(ctx, failfast, balancer.PickInfo&#123;
        Ctx:            ctx,
        FullMethodName: method,
    &#125;)
&#125;
func (pw *pickerWrapper) pick(ctx context.Context, failfast bool, info balancer.PickInfo) (transport.ClientTransport, balancer.PickResult, error) &#123;
    for &#123;
        // ...
        ch = pw.blockingCh
        p := pw.picker
        pw.mu.Unlock()


        pickResult, err := p.Pick(info)
        // ...


        acw, ok := pickResult.SubConn.(*acBalancerWrapper)
        // ...
        if t := acw.getAddrConn().getReadyTransport(); t != nil &#123;
            // ...
            return t, pickResult, nil
        &#125;
        // ...
        
    &#125;
&#125;
func (p *picker) Pick(balancer.PickInfo) (balancer.PickResult, error) &#123;
    return p.result, p.err
&#125;
</code></pre>
<h3 id="4-2-4-csAttempt-newStream"><a href="#4-2-4-csAttempt-newStream" class="headerlink" title="4.2.4 csAttempt.newStream"></a>4.2.4 csAttempt.newStream</h3><p>在 csAttempt.newStream 方法中，会通过 ClientTransport.NewStream 方法创建一个 rpc stream<br>用于后续的请求通信. 这部分放在后续单开的通信篇中再作展开.</p>
<pre><code>func (a *csAttempt) newStream() error &#123;
    cs := a.cs
    // ...
    // 构造一个 grpc stream
    s, err := a.t.NewStream(a.ctx, cs.callHdr)
    // ...
    a.s = s
    a.p = &amp;parser&#123;r: s&#125;
    return nil
&#125;
</code></pre>
<h2 id="4-3-clientStream-SendMsg"><a href="#4-3-clientStream-SendMsg" class="headerlink" title="4.3 clientStream.SendMsg"></a>4.3 clientStream.SendMsg</h2><p>发送请求是基于 clientStream.SendMsg -&gt; csAttempt.ClientTransport.sendMsg<br>来执行的，有关通信模块的细节，我们后续单开一个通信篇再作展开.</p>
<pre><code>func (cs *clientStream) SendMsg(m interface&#123;&#125;) (err error) &#123;
    // ...
    // 消息前处理，包括编码、压缩等细节
    hdr, payload, data, err := prepareMsg(m, cs.codec, cs.cp, cs.comp)
    // ...
    // 通过 csAttempt 发送请求
    op := func(a *csAttempt) error &#123;
        return a.sendMsg(m, hdr, payload, data)
    &#125;
    err = cs.withRetry(op, func() &#123; cs.bufferForRetryLocked(len(hdr)+len(payload), op) &#125;)
    // ...
    return err
&#125;
func (a *csAttempt) sendMsg(m interface&#123;&#125;, hdr, payld, data []byte) error &#123;
    cs := a.cs
    // ...
    if err := a.t.Write(a.s, hdr, payld, &amp;transport.Options&#123;Last: !cs.desc.ClientStreams&#125;); err != nil &#123;
        // ...
    &#125;
    // ...
    return nil
&#125;
</code></pre>
<h2 id="4-4-clientStream-RecvMsg"><a href="#4-4-clientStream-RecvMsg" class="headerlink" title="4.4 clientStream.RecvMsg"></a>4.4 clientStream.RecvMsg</h2><p>grpc-go客户端接收来自服务端的响应参数时，基于 clientStream.RecvMsg -&gt; csAttempt.recvMsg -&gt; recv<br>的方法链路完成，recv 方法内部会先通过 recvAndDecompress 方法接收响应并进行解压，然后调用 baseCodec.Unmarshal<br>方法，遵循特定的协议对响应进行反序列化.</p>
<p>这部分通信和编码相关的细节，我们后续单开通信篇章再作展开.</p>
<pre><code>func (cs *clientStream) RecvMsg(m interface&#123;&#125;) error &#123;
    // 通过 csAttempt 接收响应
    err := cs.withRetry(func(a *csAttempt) error &#123;
        return a.recvMsg(m, recvInfo)
    &#125;, cs.commitAttemptLocked)
    // ...
    return err
&#125;
func (a *csAttempt) recvMsg(m interface&#123;&#125;, payInfo *payloadInfo) (err error) &#123;
    cs := a.cs
    // ...
    err = recv(a.p, cs.codec, a.s, a.dc, m, *cs.callInfo.maxReceiveMessageSize, payInfo, a.decomp)
    // ...
&#125;
func recv(p *parser, c baseCodec, s *transport.Stream, dc Decompressor, m interface&#123;&#125;, maxReceiveMessageSize int, payInfo *payloadInfo, compressor encoding.Compressor) error &#123;
    d, err := recvAndDecompress(p, s, dc, maxReceiveMessageSize, payInfo, compressor)
    // ...
    if err := c.Unmarshal(d, m); err != nil &#123;
        return status.Errorf(codes.Internal, &quot;grpc: failed to unmarshal the received message: %v&quot;, err)
    &#125;
    // ...
&#125;
</code></pre>
<h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5 小结"></a>5 小结</h2><p>本文从grpc-go客户端视角出发，沿着 grpc.Dial 和 grpc.Invoke<br>两条主线进行了源码走读，整体来说代码量偏大，分析的部分较少，更多是在梳理客户端的主流程框架，为后续grpc-go通信篇的展开打好基础.</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/golang/" style="color: #ff7d73">golang</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/grpc/" style="color: #03a9f4">grpc</a>
        </span>
        
    </div>
    <a href="/2024/05/11/mdstorage/domain/network/%E5%8D%8F%E8%AE%AE/grpc-go%E5%AE%A2%E6%88%B7%E7%AB%AF/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/10/mdstorage/domain/linux/eventfd%E6%9C%BA%E5%88%B6/">
        <h2 class="post-title">eventfd机制.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/10
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="Linux-fd-系列-—-eventfd-是什么？"><a href="#Linux-fd-系列-—-eventfd-是什么？" class="headerlink" title="Linux fd 系列 — eventfd 是什么？"></a>Linux fd 系列 — eventfd 是什么？</h1><h2 id="一切皆文件，但-fd-区分类型？"><a href="#一切皆文件，但-fd-区分类型？" class="headerlink" title="一切皆文件，但 fd 区分类型？"></a><strong>一切皆文件，但 fd 区分类型？</strong></h2><p>Linux 一切皆文件，但这个文件 fd 也是有类型的，绝大部分人都知道“文件 fd”，知道 socket fd，甚至知道 pipe fd，可能都不知道<br>fd 还有这么一种叫做 <code>eventfd</code> 的类型。</p>
<h2 id="eventfd-是什么的？"><a href="#eventfd-是什么的？" class="headerlink" title="eventfd 是什么的？"></a><strong>eventfd 是什么的？</strong></h2><p>不妨拆开来看，event fd ，也就是事件 fd 类型。故名思义，就是专门用于事件通知的文件描述符（ fd ）。很多人可能没怎么用，但是用过的人都说：香<br>！</p>
<p>哪个版本引入的？</p>
<p>Linux 2.6.22</p>
<p>代码位于：<code>fs/eventfd.c</code></p>
<p>“事件传递”就是通信嘛。eventfd 不仅可以用于进程间的通信，还能用于用户态和内核态的通信。</p>
<p>思考一个小问题：我们知道“文件”里是保存东西的，eventfd 既然对应了一个“文件”，那么这个“文件”的内容是什么呢？</p>
<p><strong>划重点：eventfd 是一个计数相关的fd</strong> 。计数不为零是有<strong>可读事件</strong> 发生，<code>read</code> 之后计数会清零，<code>write</code> 则会递增计数器。</p>
<p>这个怎么理解？</p>
<p>在之前自制文件系统系列中提到过：文件系统的“文件”是抽象的概念，你看到的一切知识文件系统想让你看到的东西。比如 hellofs<br>中我们没写过任何数据，也会返回 “hello world” 的内容。这个仅仅 hook 到 read&#x2F;write 调用，然后根据逻辑返回数据而已。</p>
<p>eventfd 也是如此，eventfd 实现了 read&#x2F;write 的调用，在调用里面实现了一套计数器的逻辑。write 仅仅是加计数，read<br>是读计数，并且清零。</p>
<p>长什么样子呢？笔者找了个进程来观摩下。</p>
<pre><code>root@ubuntu:~# ll /proc/14168/fd
lrwx------ 1 root root 64 Jul 10 22:12 3 -&gt; anon_inode:[eventfd]
</code></pre>
<p>在 Linux 的 <code>/proc</code> 下每个进程都会有个目录，目录名为进程 ID 号，在这个目录能看到使用的资源信息，其中有个 fd<br>目录，就是进程打开的所有文件。看出猫腻了不？有个叫做 <code>[eventfd]</code> 的 fd 句柄。</p>
<h2 id="怎么使用它呢？"><a href="#怎么使用它呢？" class="headerlink" title="怎么使用它呢？"></a><strong>怎么使用它呢？</strong></h2><h3 id="句柄创建"><a href="#句柄创建" class="headerlink" title="句柄创建"></a><strong>句柄创建</strong></h3><pre><code>#include &lt;sys/eventfd.h&gt;
int eventfd(unsigned int initval, int flags);
</code></pre>
<p>举个栗子：</p>
<pre><code>efd = eventfd(0, 0);
if (efd == -1)
    handle_error(&quot;eventfd&quot;);
</code></pre>
<p>这样就创建出了一个 eventfd 类型的 fd 啦。会在你的 <code>/proc/$&#123;pid&#125;/fd/</code> 目录中有一个 eventfd 类型的句柄。</p>
<h3 id="eventfd-api-调用？"><a href="#eventfd-api-调用？" class="headerlink" title="eventfd api 调用？"></a><strong>eventfd api 调用？</strong></h3><p>eventfd new 出来之后，总结来说，可以对它做四个事情：</p>
<ol>
<li>可以读这个 fd；</li>
<li>可以写这个 fd；</li>
<li>可以监听这个 fd；</li>
<li>可以关闭这个 fd；</li>
</ol>
<p>我怎么知道这个知识点的？</p>
<p>因为在 Linux 内核代码中，我看到了呀。eventfd 就实现了这几个调用。</p>
<pre><code>static const struct file_operations eventfd_fops = &#123;
#ifdef CONFIG_PROC_FS
    .show_fdinfo = eventfd_show_fdinfo,
#endif
    .release = eventfd_release,
    .poll  = eventfd_poll,
    .read  = eventfd_read,
    .write  = eventfd_write,
    .llseek  = noop_llseek,
&#125;;
</code></pre>
<p>很明显就能看到以上实现的几个调用就是 eventfd 全部的内容所在。</p>
<h3 id="读写-fd"><a href="#读写-fd" class="headerlink" title="读写 fd"></a><strong>读写 fd</strong></h3><p>简单看下 eventfd 的读写究竟做了什么？</p>
<p>eventfd 对应的文件内容是一个 8 字节的数字，这个数字是 read&#x2F;write 操作维护的计数。</p>
<p>首先，write 的时候，累加计数，read 的时候读取计数，并且清零。</p>
<pre><code>uint64_t u;
ssize_t n;

// 写 eventfd，内部 buffer 必须是 8 字节大小；
n = write(efd, &amp;u, sizeof(uint64_t));

// 读 eventfd
n = read(efd, &amp;u, sizeof(uint64_t));
</code></pre>
<p>读写也就是 read&#x2F;write，读写这个 fd 很容易理解，但是请注意了，只能 8 个字节。这个读写的内容其实是计数。</p>
<p>举个栗子：如下，我们连续写 3 次</p>
<pre><code>// 写 3 次
write(efd, &amp;u /* u = 1 */ , 8)
write(efd, &amp;u /* u = 2 */ , 8)
write(efd, &amp;u /* u = 3 */ , 8)
</code></pre>
<p>你猜猜读的时候，是多少？</p>
<pre><code>read(ebd, &amp;x, 8)
</code></pre>
<p>读到的值是 6（因为 1+2+3），理解了吧。</p>
<p><img src="https://pic3.zhimg.com/v2-1aeeec8d6878ce1c1d4502a52ff01c6e_b.gif"></p>
<p><img src="/./eventfd_files/v2-1aeeec8d6878ce1c1d4502a52ff01c6e_b.jpg" alt="动图封面"></p>
<p>小结：</p>
<ol>
<li>写的时候，写进去一个 8 字节的整数，eventfd 实现的逻辑是累计计数；</li>
<li>读的时候，读到总计数，并且会清零；</li>
<li>实现在 <code>eventfd_write</code> 和 <code>eventfd_read</code> 函数中；</li>
</ol>
<h3 id="监听-fd"><a href="#监听-fd" class="headerlink" title="监听 fd"></a><strong>监听 fd</strong></h3><p>在 <strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/write">深入理解 Linux Epoll 池</a></strong> 提到过，不是所有的 fd<br>类型都可用 epoll 池来监听事件的，只有实现了 <code>file_operation-&gt;poll</code> 的调用的“文件” fd 才能被 epoll<br>管理。eventfd 刚好就实现了这个接口。</p>
<p>eventfd 是专门用来传递事件的 fd ，而 epoll 池则是专门用来管理事件的池子，它们两结合就妙了。</p>
<p>我们知道 epoll 监听的是<strong>可读可写事件</strong> 。那么你想过 eventfd 的可读可写事件是啥吗？</p>
<p>“<strong>可读可写事件</strong> ”这是个有趣的问题，我们可以去发散下，对比思考下 socket fd，文件 fd：</p>
<ul>
<li>socket fd：可以写入发送数据，那么触发可写事件，网卡数据来了，可以读，触发可读事件；</li>
<li>文件 fd：文件 fd 的可读可写事件就更有意思了，因为文件一直是可写的，所以一直都触发可写事件，文件里的数据也一直是可读的，所以一直触发可读事件。这个也是为什么类似 ext4 这种文件不实现 poll 接口的原因。<strong>因为文件 fd 一直是可读可写的，poll 监听没有任何意义；</strong></li>
</ul>
<p>回到最初问题：eventfd 呢？它的可读可写事件是什么？</p>
<p>我们之前说过，eventfd 实现的是计数的功能。所以 eventfd 计数不为 0 ，那么 fd 是可读的。</p>
<p>由于 eventfd 一直可写（可以一直累计计数），所以一直有可写事件。</p>
<p>所以，这里有个什么隐藏知识点呢？</p>
<p><strong>eventfd 如果用 epoll 监听事件，那么都是监听读事件，因为监听写事件无意义。</strong></p>
<h3 id="关闭-fd"><a href="#关闭-fd" class="headerlink" title="关闭 fd"></a><strong>关闭 fd</strong></h3><p>关闭这个很容里理解，就是不需要这个 fd 了，主动调用一把 Close ，当没有人使用的时候，内核会释放这个 fd 的资源。</p>
<h3 id="fd-的阻塞属性"><a href="#fd-的阻塞属性" class="headerlink" title="fd 的阻塞属性"></a><strong>fd 的阻塞属性</strong></h3><p>我们知道读写 fd 的时候，可能会遇到阻塞，对于 socket fd 来说，没有数据的时候来读，则会阻塞。写 buffer 满了的时候来写，则会阻塞。</p>
<p>那么对于 eventfd 呢？它的阻塞有可能是怎么样的？</p>
<p>read eventfd 的时候，<strong>如果计数器的值为 0，就会阻塞</strong> （这种就等同于没“文件”内容）。</p>
<p>这种可以设置 fd 的属性为非阻塞类型，这样读的时候，如果计数器为 0 ，返回 EAGAIN 即可，这样就不会阻塞整个系统。</p>
<h2 id="通常的用途"><a href="#通常的用途" class="headerlink" title="通常的用途"></a><strong>通常的用途</strong></h2><p>单独的 eventfd 看似平平无奇，但其实有非常重要的应用。下面列举几个小例子：</p>
<h3 id="磁盘的异步-IO（-libaio-）"><a href="#磁盘的异步-IO（-libaio-）" class="headerlink" title="磁盘的异步 IO（ libaio ）"></a><strong>磁盘的异步 IO（ libaio ）</strong></h3><p>我们之前说过，类似于 ext4 这种文件系统的文件 fd ，其实是不能用 epoll 来管理的，网络 fd 才可以。因为磁盘文件一直可读可写。</p>
<p>难道文件就自绝于此吗？用不了事件机制吗？只能同步 IO 吗？</p>
<p>非也。Linux 内核提供了一个叫做 libaio 的机制，能够同时提交多个 io 请求给内核（这种批量递交能提高优化的概率，大量IO堆积到设备的队列中时,<br>内核可以发挥 IO 调度算法的优势,比如合并 IO 等）。</p>
<p>aio 请求完成之后，走异步的事件通知。这个事件通知的原理就是把一个 eventfd 和这个 aio 的上下文绑定起来。aio 完成，就会往 eventfd<br>里面写计数，从而触发可读事件。</p>
<h3 id="kvm-的-ioeventfd-机制"><a href="#kvm-的-ioeventfd-机制" class="headerlink" title="kvm 的 ioeventfd 机制"></a><strong>kvm 的 ioeventfd 机制</strong></h3><p>QEMU 可以将 VM 特定地址关联一个 eventfd，对进行监听，当Guest 进行 IO 操作 exit 到 kvm 后，kvm 可以判断本次exit<br>是否发生在这段特定地址中，如果是则会通过使用 eventfd 进行事件通知，进行 IO 操作，这种方式对比能节省一些时间。</p>
<h3 id="还有什么朴实的用法？"><a href="#还有什么朴实的用法？" class="headerlink" title="还有什么朴实的用法？"></a><strong>还有什么朴实的用法？</strong></h3><p>最简单的例子，一个消费者和多个生产者，这种就可以借助 eventfd 优雅的完成事件通知。</p>
<p>生产者：</p>
<p>是多个线程，会把请求投递到一个 list 中，然后唤醒生产者。</p>
<pre><code>producer:
    // 投递请求到链表
    list_add( global_list, request )
    // 唤醒消费者处理
    write(eventfd, &amp;cnt /* 1 */ , 8)
</code></pre>
<p>消费者：</p>
<p>是一个线程，后台 loop 处理。使用 epoll 监听 eventfd 的可读事件，这样能做到一旦有请求入队，消费者就立马唤醒处理。</p>
<pre><code>consumer 
    // 添加 eventfd 到监听池
    epoll_ctl(ep, EPOLL_CTL_ADD, eventfd, &amp;ee);

loop:
    // 等待唤醒
    epoll_wait(ep, ... );
    
    // 读取新添加到列表里的元素个数，并且进行处理；
    n = read(eventfd, ... )
    // 遍历链表处理
    for each global_list:
        // do something
</code></pre>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><ol>
<li>Linux 一切皆文件，但 fd 各有不同；</li>
<li>eventfd 实现了 read&#x2F;write 的接口，本质是一个计数器的实现；</li>
<li>eventfd 实现了 poll 接口，所以可以和 epoll 双剑合璧，实现事件的通知管理；</li>
<li>eventfd 可以和 libaio &amp; epoll 一起，实现 Linux 下的纯异步 IO；</li>
<li>eventfd 监听可读事件才有意义；</li>
<li>ext4 这种文件 fd 一直可读可写，所以实现 poll 毫无意义。eventfd 一直可写，所以监听可写毫无意义；</li>
<li>eventfd 可以结合业务，做一个事件通知的通信机制，非常巧妙；</li>
</ol>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/05/10/mdstorage/domain/linux/eventfd%E6%9C%BA%E5%88%B6/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/10/mdstorage/domain/network/epoll%E6%9C%BA%E5%88%B6/">
        <h2 class="post-title">epoll机制.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/10
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="epoll机制-epoll-create、epoll-ctl、epoll-wait、close-的使用"><a href="#epoll机制-epoll-create、epoll-ctl、epoll-wait、close-的使用" class="headerlink" title="epoll机制:epoll_create、epoll_ctl、epoll_wait、close 的使用"></a>epoll机制:epoll_create、epoll_ctl、epoll_wait、close 的使用</h1><p>在linux的网络编程中，很长的时间都在使用select来做事件触发。在linux新的内核中，有了一种替换它的机制，就是epoll。相比于select，epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。并且，linux&#x2F;posix_types.h头文件有这样的声明：<br>#define__FD_SETSIZE 1024<br><strong>表示select最多同时监听1024个fd，当然，可以通过修改头文件再重编译内核来扩大这个数目，但这似乎并不治本。</strong></p>
<p><strong>epoll的接口非常简单，一共就三个函数：</strong><br>1.<strong>创建epoll句柄</strong><br>int epfd &#x3D; epoll_create(intsize);</p>
<p>创建一个epoll的句柄，<strong>size用来告诉内核这个监听的数目一共有多大</strong><br>。<strong>这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值</strong><br>。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看&#x2F;proc&#x2F;进程id&#x2F;fd&#x2F;，是能够看到这个fd的，<strong>所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。</strong><br>函数声明：int epoll_create(int size)<br>该 函数生成一个epoll专用的文件描述符。它其实是在内核申请一空间，用来存放你想关注的socket<br>fd上是否发生以及发生了什么事件。size就是你在这个epoll fd上能关注的最大socket<br>fd数。随你定好了。只要你有空间。可参见上面与select之不同<br>2.<strong>将被监听的描述符添加到epoll句柄或从epool句柄中删除或者对监听事件进行修改。</strong></p>
<p>*<em>函数声明：int epoll_ctl(int epfd, int op, int fd, struct epoll_event <em>event)</em></em><br><strong>该函数用于控制某个epoll文件描述符上的事件，可以注册事件，修改事件，删除事件。</strong><br><strong>参数：</strong><br><strong>epfd：由 epoll_create 生成的epoll专用的文件描述符；</strong><br><strong>op：要进行的操作例如注册事件，可能的取值EPOLL_CTL_ADD 注册、EPOLL_CTL_MOD 修 改、EPOLL_CTL_DEL 删除</strong></p>
<p><strong>fd：关联的文件描述符；</strong><br><strong>event：指向epoll_event的指针；</strong><br><strong>如果调用成功返回0,不成功返回-1</strong></p>
<p>int epoll_ctl(int epfd, int<strong>op</strong> , int fd, struct epoll_event*event);</p>
<p>epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。</p>
<p>第一个参数是epoll_create()的返回值，</p>
<p>第二个参数表示动作，用三个宏来表示：<br>EPOLL_CTL_ADD： 注册新的fd到epfd中；<br>EPOLL_CTL_MOD： 修改已经注册的fd的监听事件；<br>EPOLL_CTL_DEL： 从epfd中删除一个fd；<br>第三个参数是需要监听的fd，</p>
<p>第四个参数是告诉内核需要监听什么事件，struct epoll_event结构如下：</p>
<p>typedef union epoll_data {<br>void *ptr;<br>int fd;<br>__uint32_t u32;<br>__uint64_t u64;<br>} epoll_data_t;</p>
<p>struct epoll_event {<br>__uint32_t events; <em>&#x2F;* Epoll events *&#x2F;</em><br>epoll_data_t data; <em>&#x2F;* User data variable *&#x2F;</em><br>};</p>
<p>events可以是以下几个宏的集合：<br>EPOLLIN： 触发该事件，表示对应的文件描述符上有可读数据。(包括对端SOCKET正常关闭)；<br>EPOLLOUT： 触发该事件，表示对应的文件描述符上可以写数据；<br>EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；<br>EPOLLERR： 表示对应的文件描述符发生错误；<br>EPOLLHUP： 表示对应的文件描述符被挂断；<br>EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。<br>EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里。<br>如：<br>struct epoll_event ev;<br>&#x2F;&#x2F;设置与要处理的事件相关的文件描述符<br>ev.data.fd&#x3D;listenfd;<br>&#x2F;&#x2F;设置要处理的事件类型<br>ev.events&#x3D;EPOLLIN|EPOLLET;<br>&#x2F;&#x2F;注册epoll事件<br>epoll_ctl(epfd,EPOLL_CTL_ADD,listenfd,&amp;ev);<br><strong>3.等待事件触发，当超过timeout还没有事件触发时，就超时。</strong></p>
<p><strong>int epoll_wait(int epfd, struct epoll_event * events, intmaxevents, int<br>timeout);</strong><br>等待事件的产生，类似于select()调用。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大(数组成员的个数)，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。</p>
<p>该函数返回需要处理的事件数目，如返回0表示已超时。</p>
<h2 id="返回的事件集合在events数组中，数组中实际存放的成员个数是函数的返回值。返回0表示已经超时。函数声明-int-epoll-wait-int-epfd-struct-epoll-event-events-int-maxevents-inttimeout-该函数用于轮询I-O事件的发生；参数：epfd-由epoll-create-生成的epoll专用的文件描述符；epoll-event-用于回传代处理事件的数组；maxevents-每次能处理的事件数；timeout-等待I-O事件发生的超时值-单位我也不太清楚-；-1相当于阻塞，0相当于非阻塞。一般用-1即可返回发生事件数。epoll-wait运行的原理是等侍注册在epfd上的socket-fd的事件的发生，如果发生则将发生的sokct-fd和事件类型放入到events数组中。并-且将注册在epfd上的socket-fd的事件类型给清空-，所以如果下一个循环你还要关注这个socketfd的话，则需要用epoll-ctl-epfd-EPOLL-CTL-MOD-listenfd-ev-来重新设置socketfd的事件类型。这时不用EPOLL-CTL-ADD-因为socket-fd并未清空，只是事件类型清空。这一步非常重要。"><a href="#返回的事件集合在events数组中，数组中实际存放的成员个数是函数的返回值。返回0表示已经超时。函数声明-int-epoll-wait-int-epfd-struct-epoll-event-events-int-maxevents-inttimeout-该函数用于轮询I-O事件的发生；参数：epfd-由epoll-create-生成的epoll专用的文件描述符；epoll-event-用于回传代处理事件的数组；maxevents-每次能处理的事件数；timeout-等待I-O事件发生的超时值-单位我也不太清楚-；-1相当于阻塞，0相当于非阻塞。一般用-1即可返回发生事件数。epoll-wait运行的原理是等侍注册在epfd上的socket-fd的事件的发生，如果发生则将发生的sokct-fd和事件类型放入到events数组中。并-且将注册在epfd上的socket-fd的事件类型给清空-，所以如果下一个循环你还要关注这个socketfd的话，则需要用epoll-ctl-epfd-EPOLL-CTL-MOD-listenfd-ev-来重新设置socketfd的事件类型。这时不用EPOLL-CTL-ADD-因为socket-fd并未清空，只是事件类型清空。这一步非常重要。" class="headerlink" title="返回的事件集合在events数组中，数组中实际存放的成员个数是函数的返回值。返回0表示已经超时。函数声明:int epoll_wait(int epfd,struct epoll_event * events,int maxevents,inttimeout)该函数用于轮询I&#x2F;O事件的发生；参数：epfd:由epoll_create 生成的epoll专用的文件描述符；epoll_event:用于回传代处理事件的数组；maxevents:每次能处理的事件数；timeout:等待I&#x2F;O事件发生的超时值(单位我也不太清楚)；-1相当于阻塞，0相当于非阻塞。一般用-1即可返回发生事件数。epoll_wait运行的原理是等侍注册在epfd上的socket fd的事件的发生，如果发生则将发生的sokct fd和事件类型放入到events数组中。并 且将注册在epfd上的socket fd的事件类型给清空 ，所以如果下一个循环你还要关注这个socketfd的话，则需要用epoll_ctl(epfd,EPOLL_CTL_MOD,listenfd,&amp;ev)来重新设置socketfd的事件类型。这时不用EPOLL_CTL_ADD,因为socket fd并未清空，只是事件类型清空。这一步非常重要。  "></a>返回的事件集合在events数组中，数组中实际存放的成员个数是函数的返回值。返回0表示已经超时。<br>函数声明:int epoll_wait(int epfd,struct epoll_event * events,int maxevents,int<br>timeout)<br>该函数用于轮询I&#x2F;O事件的发生；<br>参数：<br>epfd:由epoll_create 生成的epoll专用的文件描述符；<br>epoll_event:用于回传代处理事件的数组；<br>maxevents:每次能处理的事件数；<br>timeout:等待I&#x2F;O事件发生的超时值(单位我也不太清楚)；-1相当于阻塞，0相当于非阻塞。一般用-1即可<br>返回发生事件数。<br>epoll_wait运行的原理是<br>等侍注册在epfd上的socket fd的事件的发生，如果发生则将发生的sokct fd和事件类型放入到events数组中。<br><strong>并 且将注册在epfd上的socket fd的事件类型给清空</strong> ，所以如果下一个循环你还要关注这个socket<br>fd的话，则需要用epoll_ctl(epfd,EPOLL_CTL_MOD,listenfd,&amp;ev)来重新设置socket<br>fd的事件类型。这时不用EPOLL_CTL_ADD,因为socket fd并未清空，只是事件类型清空。这一步非常重要。  </h2><p>从man手册中，得到ET和LT的具体描述如下<br>EPOLL事件有两种模型：<br>Edge Triggered(ET) &#x2F;&#x2F;<strong>高速工作方式，错误率比较大，只支持no_block socket (非阻塞socket)</strong><br>LevelTriggered(LT) &#x2F;&#x2F;<strong>缺省工作方式，即默认的工作方式,支持blocksocket和no_blocksocket，错误率比较小。</strong></p>
<p>假如有这样一个例子：(LT方式，即默认方式下，内核会继续通知，可以读数据，ET方式，内核不会再通知，可以读数据)<br>1.我们已经把一个用来从管道中读取数据的文件句柄(RFD)添加到epoll描述符<br>2. 这个时候从管道的另一端被写入了2KB的数据<br>3. 调用epoll_wait(2)，并且它会返回RFD，说明它已经准备好读取操作<br>4. 然后我们读取了1KB的数据<br>5. 调用epoll_wait(2)……</p>
<p><strong>Edge Triggered工作模式：</strong><br>如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait(2)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候ET工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。在上面的例子中，会有一个事件产生在RFD句柄上，因为在第2步执行了一个写操作，然后，事件将会在第3步被销毁。因为第4步的读取操作没有读空文件输入缓冲区内的数据，因此我们在第5步调用epoll_wait(2)完成后，是否挂起是不确定的。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读&#x2F;阻塞写操作把处理多个文件描述符的任务饿死。最好以下面的方式调用ET模式的epoll接口，<strong>在后面会介绍避免可能的缺陷。(LT方式可以解决这种缺陷)</strong>  </p>
<p>i 基于非阻塞文件句柄<br>ii 只有当read(2)或者write(2)返回EAGAIN时(认为读完)才需要挂起，等待。<strong>但这并不是说每次read()时都需要循环读，直到读到产生一个EAGAIN才认为此次事件处理完成，当read()返回的读到的数据长度小于请求的数据长度时(即小于sizeof(buf))，就可以确定此时缓冲中已没有数据了，也就可以认为此事读事件已处理完成。</strong></p>
<p><strong>Level Triggered工作模式 (默认的工作方式)</strong><br>相反的，以LT方式调用epoll接口的时候，它就相当于一个速度比较快的poll(2)，并且无论后面的数据是否被使用，因此他们具有同样的职能。因为即使使用ET模式的epoll，在收到多个chunk的数据的时候仍然会产生多个事件。调用者可以设定EPOLLONESHOT标志，在epoll_wait(2)收到事件后epoll会与事件关联的文件句柄从epoll描述符中禁止掉。因此当EPOLLONESHOT设定后，使用带有EPOLL_CTL_MOD标志的epoll_ctl(2)处理文件句柄就成为调用者必须作的事情。</p>
<p><strong>然后详细解释ET, LT:</strong><br><strong>&#x2F;&#x2F;没有对就绪的fd进行IO操作，内核会不断的通知。</strong><br>LT(leveltriggered)是缺省的工作方式，并且同时支持block和no-<br>blocksocket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，<strong>内核还是会继续通知你的</strong><br>，所以，这种模式编程出错误可能性要小一点。传统的select&#x2F;poll都是这种模型的代表。<br><strong>&#x2F;&#x2F;没有对就绪的fd进行IO操作，内核不会再进行通知。</strong><br>ET(edge-triggered)是高速工作方式，只支持no-blocksocket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(<strong>比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK错误）。但是请注意，如果一直不</strong><br>对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only<br>once),<strong><em>不过在TCP协议中，ET模式的加速效用仍需要更多的benchmark确认（这句话不理解）。</em></strong></p>
<p>另外，当使用epoll的ET模型(epoll的非默认工作方式)来工作时，当产生了一个EPOLLIN事件后，<br>读数据的时候需要考虑的是当recv()返回的大小如果等于要求的大小，即sizeof(buf)，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取：<br>while(rs) &#x2F;&#x2F;ET模型<br>{<br>buflen &#x3D; recv(activeevents[i].data.fd, buf, sizeof(buf), 0);<br>if(buflen &lt; 0)<br>{<br>&#x2F;&#x2F;由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读<br>&#x2F;&#x2F; 在这里就当作是该次事件已处理处.<br>if(errno&#x3D;&#x3D; EAGAIN || errno &#x3D;&#x3D; EINT)<br>&#x2F;&#x2F;即当buflen&lt;0且errno&#x3D;EAGAIN时，表示没有数据了。(读&#x2F;写都是这样)<br>break;<br>else</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/05/10/mdstorage/domain/network/epoll%E6%9C%BA%E5%88%B6/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/09/mdstorage/domain/network/conn%E7%9A%84%E5%B0%81%E8%A3%85%E5%92%8C%E5%AE%9E%E7%8E%B0/">
        <h2 class="post-title">conn的封装和实现.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/9
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>经典 Conn 的封装和实现解析</p>
<h2 id="conn-的解析"><a href="#conn-的解析" class="headerlink" title="conn 的解析"></a>conn 的解析</h2><p>conn 实现基于流的 网络连接，通用的面向流的网络连接，多个Goroutine可以同时调用Conn上的方法。<br>主要通过Read(b []byte)读取数据，Write(b [byte]) 写数据 及Close() 关闭连接。</p>
<p>Conn 接口定义</p>
<pre><code class="Go">// Conn is a generic stream-oriented network connection.
//
// Multiple goroutines may invoke methods on a Conn simultaneously.
type Conn interface &#123;
    // Read reads data from the connection.
    // Read can be made to time out and return an error after a fixed
    // time limit; see SetDeadline and SetReadDeadline.
    Read(b []byte) (n int, err error)

    // Write writes data to the connection.
    // Write can be made to time out and return an error after a fixed
    // time limit; see SetDeadline and SetWriteDeadline.
    Write(b []byte) (n int, err error)

    // Close closes the connection.
    // Any blocked Read or Write operations will be unblocked and return errors.
    Close() error

    // LocalAddr returns the local network address, if known.
    LocalAddr() Addr

    // RemoteAddr returns the remote network address, if known.
    RemoteAddr() Addr

    // SetDeadline sets the read and write deadlines associated
    // with the connection. It is equivalent to calling both
    // SetReadDeadline and SetWriteDeadline.
    //
    // A deadline is an absolute time after which I/O operations
    // fail instead of blocking. The deadline applies to all future
    // and pending I/O, not just the immediately following call to
    // Read or Write. After a deadline has been exceeded, the
    // connection can be refreshed by setting a deadline in the future.
    //
    // If the deadline is exceeded a call to Read or Write or to other
    // I/O methods will return an error that wraps os.ErrDeadlineExceeded.
    // This can be tested using errors.Is(err, os.ErrDeadlineExceeded).
    // The error&#39;s Timeout method will return true, but note that there
    // are other possible errors for which the Timeout method will
    // return true even if the deadline has not been exceeded.
    //
    // An idle timeout can be implemented by repeatedly extending
    // the deadline after successful Read or Write calls.
    //
    // A zero value for t means I/O operations will not time out.
    SetDeadline(t time.Time) error

    // SetReadDeadline sets the deadline for future Read calls
    // and any currently-blocked Read call.
    // A zero value for t means Read will not time out.
    SetReadDeadline(t time.Time) error

    // SetWriteDeadline sets the deadline for future Write calls
    // and any currently-blocked Write call.
    // Even if write times out, it may return n &gt; 0, indicating that
    // some of the data was successfully written.
    // A zero value for t means Write will not time out.
    SetWriteDeadline(t time.Time) error
&#125;
</code></pre>
<p>conn 实现</p>
<pre><code class="Bash">type conn struct &#123;
    fd *netFD
&#125;
</code></pre>
<p>fd 实现</p>
<pre><code class="Go">socketFunc        func(int, int, int) (int, error)  = syscall.Socket
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/05/09/mdstorage/domain/network/conn%E7%9A%84%E5%B0%81%E8%A3%85%E5%92%8C%E5%AE%9E%E7%8E%B0/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/09/mdstorage/domain/network/Socket%E7%9A%84%E5%B0%81%E8%A3%85/">
        <h2 class="post-title">Socket的封装.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/9
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="Socket与系统调用深度分析-https-www-cnblogs-com-RichardTAO-p-12070548-html-“发布于"><a href="#Socket与系统调用深度分析-https-www-cnblogs-com-RichardTAO-p-12070548-html-“发布于" class="headerlink" title="[ Socket与系统调用深度分析 ](https://www.cnblogs.com/RichardTAO/p/12070548.html “发布于"></a>[ Socket与系统调用深度分析 ](<a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html">https://www.cnblogs.com/RichardTAO/p/12070548.html</a> “发布于</h1><p>2019-12-19 22:08”)</p>
<p>目录</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8">系统调用</a><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E7%9A%84%E8%BF%87%E7%A8%8B">系统调用的过程</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E7%A4%BA%E4%BE%8B%E5%88%86%E6%9E%90">示例分析</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#socket%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90">Socket系统调用分析</a><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#sock_map_fd">sock_map_fd</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#sock_create">sock_create</a></li>
</ul>
</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90">实验代码分析</a><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF">服务器端</a><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#initializeservice%E6%96%B9%E6%B3%95">InitializeService方法</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#servicestart%E6%96%B9%E6%B3%95">ServiceStart方法</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#recvmsg%E4%B8%8Esendmsg%E6%96%B9%E6%B3%95">RecvMsg与SendMsg方法</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#servicestop%E4%B8%8Eshutdownservice%E6%96%B9%E6%B3%95">ServiceStop与ShutdownService方法</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E5%AE%A2%E6%88%B7%E7%AB%AF">客户端</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E8%B7%9F%E8%B8%AAsocket%E7%9B%B8%E5%85%B3%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E7%9A%84%E5%86%85%E6%A0%B8%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0">跟踪socket相关系统调用的内核处理函数</a><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E5%88%9D%E5%A7%8B%E5%8C%96">系统调用初始化</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#socket%E7%9B%B8%E5%85%B3%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8">socket相关的系统调用</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12070548.html#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5">参考链接</a></li>
</ul>
<h1 id="1-0-系统调用"><a href="#1-0-系统调用" class="headerlink" title="1 |**0**系统调用"></a><strong><em>1</em></strong> |**<em>0</em>**<strong>系统调用</strong></h1><h2 id="1-1-系统调用的过程"><a href="#1-1-系统调用的过程" class="headerlink" title="1 |**1**系统调用的过程"></a><strong><em>1</em></strong> |**<em>1</em>**<strong>系统调用的过程</strong></h2><p>系统调用的过程如下：</p>
<ul>
<li>用户程序</li>
<li>C库（API）：INT 0x80</li>
<li>system_call</li>
<li>系统调用服务例程</li>
<li>内核程序</li>
</ul>
<blockquote>
<p>说明：</p>
<ol>
<li>我们常说的用户 API 其实就是系统提供的 C 库；</li>
<li>系统调用是通过软中断指令 INT 0x80 实现的，而这条 INT 0x80 指令就被封装在 C 库的函数中。</li>
<li>INT 0x80 这条指令的执行会让系统跳转到一个预设的内核空间地址，它指向系统调用处理程序，即 system_call 函数</li>
</ol>
</blockquote>
<p>下图为系统调用具体的流程。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219215912966-312816293.jpg"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219215912966-312816293.jpg"></a></p>
<p>值得一提的是：系统调用处理程序 system_call 并不是系统调用服务例程。</p>
<p>系统调用服务例程是对一个具体的系统调用的内核实现函数，而系统调用处理程序是在执行系统调用服务例程之前的一个引导过程，是针对 INT 0x80<br>这条指令，面向所有的系统调用的。</p>
<p>简单来讲，执行任何系统调用，都是先通过调用 C 库中的函数，这个函数里面就会有软中断 INT 0x80 语句，然后转到执行系统调用处理程序<br>system_call，system_call 再根据具体的系统调用号转到执行具体的系统调用服务例程。</p>
<hr>
<blockquote>
<p>那么，system_call 函数是怎么找到具体的系统调用服务例程的呢？</p>
</blockquote>
<p>答案是：通过系统调用号查找系统调用表 sys_call_table。</p>
<p>软中断指令 INT 0x80 执行时，系统调用号会被放入 eax 寄存器中，system_call 函数可以读取 eax 寄存器获取，然后将其乘以<br>4，生成偏移地址，然后以 sys_call_table 为基址，基址加上偏移地址，就可以得到具体的系统调用服务例程的地址了！</p>
<p>然后就可以根据这个地址找到对应的系统调用服务例程了。</p>
<hr>
<blockquote>
<p>系统调用可以被进程抢占、进入阻塞状态</p>
</blockquote>
<p>其原因在于：</p>
<p>系统调用通过软中断 INT 0x80 陷入内核，跳转到系统调用处理程序 system_call<br>函数，然后执行相应的服务例程。但是由于是代表用户进程，所以这个执行过程并不属于中断上下文，而是进程上下文。</p>
<p>因此，系统调用执行过程中，可以访问用户进程的许多信息，可以被其他进程抢占，可以休眠。<br>当系统调用完成后，把控制权交回到发起调用的用户进程前，内核会有一次调度。如果发现有优先级更高的进程或当前进程的时间片用完，那么会选择优先级更高的进程或重新选择进程执行。</p>
<h2 id="1-2-示例分析"><a href="#1-2-示例分析" class="headerlink" title="1 |**2**示例分析"></a><strong><em>1</em></strong> |**<em>2</em>**<strong>示例分析</strong></h2><p>了解系统调用的过程之后，我们举个例子来分析用户态是如何陷入到内核态的。</p>
<p>由于本次实验的目的是调研<code>socket</code>相关的系统调用，因此我们以一个<code>bind</code>绑定事件作为例子。</p>
<p>__</p>
<pre><code># include &lt;sys/socket.h&gt;
#include &lt;sys/un.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

#define MY_SOCK_PATH &quot;/somepath&quot;
#define LISTEN_BACKLOG 50

#define handle_error(msg) \
    do &#123; perror(msg); exit(EXIT_FAILURE); &#125; while (0)

int main(int argc, char *argv[])
&#123;
    int sfd, cfd;
    struct sockaddr_un my_addr, peer_addr;
    socklen_t peer_addr_size;

    sfd = socket(AF_UNIX, SOCK_STREAM, 0);
    if (sfd == -1)
        handle_error(&quot;socket&quot;);

    memset(&amp;my_addr, 0, sizeof(struct sockaddr_un));
                        /* Clear structure */
    my_addr.sun_family = AF_UNIX;
    strncpy(my_addr.sun_path, MY_SOCK_PATH,
            sizeof(my_addr.sun_path) - 1);

    if (bind(sfd, (struct sockaddr *) &amp;my_addr,
            sizeof(struct sockaddr_un)) == -1)
        handle_error(&quot;bind&quot;);

   if (listen(sfd, LISTEN_BACKLOG) == -1)
       handle_error(&quot;listen&quot;);
</code></pre>
<blockquote>
<p>在用户态我们调用了<code>bind</code>函数，它声明在<code>&lt;sys/socket.h&gt;</code>。</p>
</blockquote>
<p>__</p>
<pre><code>/* Give the socket FD the local address ADDR (which is LEN bytes long).  */
extern int bind (int __fd, __CONST_SOCKADDR_ARG __addr, socklen_t __len)
     __THROW;
</code></pre>
<p>我们之前提到过，用户态进程只需要调用库函数即可，而不用管这个函数是如何实现的。</p>
<blockquote>
<p><code>linux</code>中这些具体函数的实现则由<code>glibc</code>统一提供。它定义在<code>glibc-2.23/sysdeps/unix/sysv/linux/bind.c</code></p>
</blockquote>
<p>__</p>
<pre><code>int __bind (int fd, __CONST_SOCKADDR_ARG addr, socklen_t len)
&#123;
#ifdef __ASSUME_BIND_SYSCALL
  return INLINE_SYSCALL (bind, 3, fd, addr.__sockaddr__, len);
#else
  return SOCKETCALL (bind, fd, addr.__sockaddr__, len, 0, 0, 0);
#endif
&#125;
weak_alias (__bind, bind)
</code></pre>
<p>在<code>syscall</code>之前需要先将参数传入寄存器。</p>
<p>之前，如我们之前分析的一样：使用0x80中断去陷入内核，并将返回值存放到eax寄存器中，通常0表示成功。</p>
<blockquote>
<p>syscall的name为<code>__NR_##name</code>，在本例中即为<code>__NR_bind</code>。</p>
</blockquote>
<p>其定义在<code>/usr/include/asm/unistd_64.h</code>中。</p>
<p>__</p>
<pre><code># define __NR_bind 49
#define __NR_listen 50
#define __NR_getsockname 51
</code></pre>
<p><strong>这样，用户态和内核态通过系统调用号（49）来确定本次系统调用是哪个功能。</strong></p>
<h2 id="1-3-Socket系统调用分析"><a href="#1-3-Socket系统调用分析" class="headerlink" title="1 |**3**Socket系统调用分析"></a><strong><em>1</em></strong> |**<em>3</em>**<strong>Socket系统调用分析</strong></h2><blockquote>
<p>Socket系统调用主要完成socket的创建，必要字段的初始化，关联传输控制块，绑定文件等任务，完成返回socket绑定的文件描述符。</p>
</blockquote>
<p>socket的调用关系如下：</p>
<p>__</p>
<pre><code>/** * sys_socket
 *   |--&gt;sock_create
 *   |      |--&gt;__sock_create
 *   |            |--&gt;inet_create           
 *   |--&gt;sock_map_fd
 */
</code></pre>
<h3 id="sock-map-fd"><a href="#sock-map-fd" class="headerlink" title="sock_map_fd"></a>sock_map_fd</h3><p>该函数的主要功能在于：负责分配文件，并实现与socket的绑定。</p>
<p>__</p>
<pre><code>/* 套接口与文件描述符绑定 */
static int sock_map_fd(struct socket *sock, int flags)
&#123;
    struct file *newfile;
    /* 获取未使用的文件描述符 */
    int fd = get_unused_fd_flags(flags);
    if (unlikely(fd &lt; 0))
        return fd;
 
    /* 分配socket文件 */
    newfile = sock_alloc_file(sock, flags, NULL);
    if (likely(!IS_ERR(newfile))) &#123;
        /* fd和文件进行绑定 */
        fd_install(fd, newfile);
        return fd;
    &#125;
 
    /* 释放fd */
    put_unused_fd(fd);
    return PTR_ERR(newfile);
&#125;
</code></pre>
<h3 id="sock-create"><a href="#sock-create" class="headerlink" title="sock_create"></a>sock_create</h3><p>其主要功能为：负责创建<code>socket</code>，并进行必要的初始化工作。</p>
<p>__</p>
<pre><code>int sock_create(int family, int type, int protocol, struct socket **res)
&#123;
    return __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, 0);
&#125;
</code></pre>
<p>之后，<code>sock_create</code>调用<code>__socket_create</code>函数来进行必要的检查项，并创建和初始化<code>socket</code>。</p>
<p>在<code>__socket_create</code>函数中，调用对应协议族的<code>pf-&gt;create</code>函数来创建传输控制块，并且与socket进行关联。</p>
<p>__</p>
<pre><code>/* 创建socket */
int __sock_create(struct net *net, int family, int type, int protocol,
             struct socket **res, int kern)
&#123;
    int err;
    struct socket *sock;
    const struct net_proto_family *pf;

    /*
     *      Check protocol is in range
     */
    /* 检查协议族 */
    if (family &lt; 0 || family &gt;= NPROTO)
        return -EAFNOSUPPORT;

    /* 检查类型 */
    if (type &lt; 0 || type &gt;= SOCK_MAX)
        return -EINVAL;

    /* Compatibility.

       This uglymoron is moved from INET layer to here to avoid
       deadlock in module load.
     */
    /* ipv4协议族的packet已经废除，检测到，则替换成packet协议族 */
    if (family == PF_INET &amp;&amp; type == SOCK_PACKET) &#123;
        pr_info_once(&quot;%s uses obsolete (PF_INET,SOCK_PACKET)\n&quot;,
                 current-&gt;comm);
        family = PF_PACKET;
    &#125;

    /* 安全模块检查套接口 */
    err = security_socket_create(family, type, protocol, kern);
    if (err)
        return err;

    /*
     *    Allocate the socket and allow the family to set things up. if
     *    the protocol is 0, the family is instructed to select an appropriate
     *    default.
     */
    /* 分配socket，内部和inode已经绑定 */
    sock = sock_alloc();
    if (!sock) &#123;
        net_warn_ratelimited(&quot;socket: no more sockets\n&quot;);
        return -ENFILE;    /* Not exactly a match, but its the
                   closest posix thing */
    &#125;

    /* 设定类型 */
    sock-&gt;type = type;

#ifdef CONFIG_MODULES
    /* Attempt to load a protocol module if the find failed.
     *
     * 12/09/1996 Marcin: But! this makes REALLY only sense, if the user
     * requested real, full-featured networking support upon configuration.
     * Otherwise module support will break!
     */
    if (rcu_access_pointer(net_families[family]) == NULL)
        request_module(&quot;net-pf-%d&quot;, family);
#endif

    rcu_read_lock();
    /* 找到协议族 */
    pf = rcu_dereference(net_families[family]);
    err = -EAFNOSUPPORT;
    if (!pf)
        goto out_release;

    /*
     * We will call the -&gt;create function, that possibly is in a loadable
     * module, so we have to bump that loadable module refcnt first.
     */
     /* 增加模块的引用计数 */
    if (!try_module_get(pf-&gt;owner))
        goto out_release;

    /* Now protected by module ref count */
    rcu_read_unlock();

    /* 调用协议族的创建函数 */
    err = pf-&gt;create(net, sock, protocol, kern);
    if (err &lt; 0)
        goto out_module_put;

    /*
     * Now to bump the refcnt of the [loadable] module that owns this
     * socket at sock_release time we decrement its refcnt.
     */
    if (!try_module_get(sock-&gt;ops-&gt;owner))
        goto out_module_busy;

    /*
     * Now that we&#39;re done with the -&gt;create function, the [loadable]
     * module can have its refcnt decremented
     */
    module_put(pf-&gt;owner);
    err = security_socket_post_create(sock, family, type, protocol, kern);
    if (err)
        goto out_sock_release;
    *res = sock;

    return 0;

out_module_busy:
    err = -EAFNOSUPPORT;
out_module_put:
    sock-&gt;ops = NULL;
    module_put(pf-&gt;owner);
out_sock_release:
    sock_release(sock);
    return err;

out_release:
    rcu_read_unlock();
    goto out_sock_release;
&#125;
EXPORT_SYMBOL(__sock_create);
</code></pre>
<p>对于<code>PF_INET</code>协议族来讲，上述的<code>pf-&gt;create</code>函数将调用<code>inet_create</code>函数。</p>
<p>熟悉设备驱动的同学应该都知道这些操作被定义在<code>file_operations</code>结构中。</p>
<p>__</p>
<pre><code>static const struct net_proto_family inet_family_ops = &#123;
    .family = PF_INET,
    .create = inet_create,
    .owner    = THIS_MODULE,
&#125;;
</code></pre>
<p>最后，在<code>inet_create</code>函数中完成创建传输控制块，并且将socket与传输控制块进行关联。至于该函数代码这里不再展开分析。</p>
<h1 id="2-0-实验代码分析"><a href="#2-0-实验代码分析" class="headerlink" title="2 |**0**实验代码分析"></a><strong><em>2</em></strong> |**<em>0</em>**<strong>实验代码分析</strong></h1><p>在<a target="_blank" rel="noopener" href="https://www.cnblogs.com/RichardTAO/p/12021971.html">上一次实验</a>中，我们已经将老师提供的客户端以及服务器端的程序集成到了<code>MenuOS</code>中，可以在<code>qemu</code>虚拟机中使用<code>replyhi</code>和<code>hello</code>两个命令。</p>
<p>但是对于这两个程序的内部实现还没有深入分析，这里我们通过阅读代码来找出这两个程序在执行时所使用到的<code>Socket API</code>。</p>
<h2 id="2-1-服务器端"><a href="#2-1-服务器端" class="headerlink" title="2 |**1**服务器端"></a><strong><em>2</em></strong> |**<em>1</em>**<strong>服务器端</strong></h2><blockquote>
<p>首先，分析一下服务器端的代码。</p>
<p>在我们使用<code>replyhi</code>命令时，会执行<code>main.c</code>文件中的StartReplyhi函数。</p>
</blockquote>
<p>__</p>
<pre><code>MenuConfig( &quot;replyhi&quot;, &quot;Reply hi TCP Service&quot;, StartReplyhi);
</code></pre>
<blockquote>
<p>继续阅读<code>StartReplyhi</code>函数的代码。</p>
<p>可以发现，在满足连接条件的判断语句中，程序继续调用了Replyhi函数。</p>
</blockquote>
<p>__</p>
<pre><code>int StartReplyhi(int argc, char *argv[])
&#123;
    int pid;
    /* fork another process */
    pid = fork();
    if (pid &lt; 0)
    &#123;
        /* error occurred */
        fprintf(stderr, &quot;Fork Failed!&quot;);
        exit(-1);
    &#125;
    else if (pid == 0)
    &#123;
        /*	 child process 	*/
        Replyhi();
        printf(&quot;Reply hi TCP Service Started!\n&quot;);
    &#125;
    else
    &#123;
        /* 	parent process	 */
        printf(&quot;Please input hello...\n&quot;);
    &#125;
&#125;
</code></pre>
<blockquote>
<p>查看<code>Replyhi</code>函数的实现。</p>
<p>在函数中总共调用了六个方法，从字面意思来看，不难推测出它们的具体含义分别是：初始化服务、启动服务、接收消息、发送消息、停止服务以及关闭服务。</p>
<p>这些方法都在头文件<code>syswrapper.h</code>中定义。</p>
<p>下面将对这些方法逐一展开分析。</p>
</blockquote>
<p>__</p>
<pre><code>int Replyhi()
&#123;
    char szBuf[MAX_BUF_LEN] = &quot;\0&quot;;
    char szReplyMsg[MAX_BUF_LEN] = &quot;hi\0&quot;;
    InitializeService();
    while (1)
    &#123;
        ServiceStart();
        RecvMsg(szBuf);
        SendMsg(szReplyMsg);
        ServiceStop();
    &#125;
    ShutdownService();
    return 0;
&#125;
</code></pre>
<h3 id="InitializeService方法"><a href="#InitializeService方法" class="headerlink" title="InitializeService方法"></a>InitializeService方法</h3><p>__</p>
<pre><code># define InitializeService()                             \
        PrepareSocket(IP_ADDR,PORT);                    \
        InitServer();
</code></pre>
<p>可以看到，在头文件中使用宏定义的方式将其具体实现私有化。</p>
<blockquote>
<p>根据第一部分的socket创建过程分析，我们可以很自然地理解<code>PrepareSocket</code>函数就是用于创建服务器端socket：在定义了<code>serveraddr</code>中的协议族、端口、IP地址信息后，通过<code>socket</code>这一系统调用完成服务器端socket的创建，并返回文件描述符信息。</p>
</blockquote>
<blockquote>
<p><code>InitServer</code>函数则是调用<code>bind</code>和<code>PrepareSocket</code>函数中得到的文件描述符完成绑定，绑定成功后，服务器进入监听状态，即等待客户端的连接，通过调用<code>listen</code>来实现监听。</p>
</blockquote>
<p>__</p>
<pre><code># define PrepareSocket(addr,port)                        \
        int sockfd = -1;                                \
        struct sockaddr_in serveraddr;                  \
        struct sockaddr_in clientaddr;                  \
        socklen_t addr_len = sizeof(struct sockaddr);   \
        serveraddr.sin_family = AF_INET;                \
        serveraddr.sin_port = htons(port);              \
        serveraddr.sin_addr.s_addr = inet_addr(addr);   \
        memset(&amp;serveraddr.sin_zero, 0, 8);             \
        sockfd = socket(PF_INET,SOCK_STREAM,0);
        
#define InitServer()                                    \
        int ret = bind( sockfd,                         \
                        (struct sockaddr *)&amp;serveraddr, \
                        sizeof(struct sockaddr));       \
        if(ret == -1)                                   \
        &#123;                                               \
            fprintf(stderr,&quot;Bind Error,%s:%d\n&quot;,        \
                            __FILE__,__LINE__);         \
            close(sockfd);                              \
            return -1;                                  \
        &#125;                                               \
        listen(sockfd,MAX_CONNECT_QUEUE); 
</code></pre>
<p>因此，在初始化服务中我们所使用到的<code>Socket API</code>共有三个，分别为：<strong>socket，bind，listen</strong> 。</p>
<h3 id="ServiceStart方法"><a href="#ServiceStart方法" class="headerlink" title="ServiceStart方法"></a>ServiceStart方法</h3><blockquote>
<p>在调用初始化服务方法之后，我们完成了服务器端socket的创建、绑定以及监听。</p>
<p>因此，在该方法的实现中，我们所实现的是接收客户端所发出的连接请求，通过调用<code>accept</code>来实现，可以看到该函数的参数中<code>clientaddr</code>则标明服务器与哪个客户端进行连接。</p>
</blockquote>
<p>__</p>
<pre><code># define ServiceStart()                                  \
        int newfd = accept( sockfd,                     \
                    (struct sockaddr *)&amp;clientaddr,     \
                    &amp;addr_len);                         \
        if(newfd == -1)                                 \
        &#123;                                               \
            fprintf(stderr,&quot;Accept Error,%s:%d\n&quot;,      \
                            __FILE__,__LINE__);         \
        &#125;   
</code></pre>
<p>因此，在启动服务中我们所使用到的<code>Socket API</code>只有一个：<strong>accept</strong> 。</p>
<h3 id="RecvMsg与SendMsg方法"><a href="#RecvMsg与SendMsg方法" class="headerlink" title="RecvMsg与SendMsg方法"></a>RecvMsg与SendMsg方法</h3><blockquote>
<p>与客户端经历三次握手建立TCP连接之后，就可以与客户端进行消息的发送与接收。</p>
<p>代码的实现与之前类似，其过程就不再赘述。不过，需要注意的是在这两个方法中，都包含<code>newfd</code>这个参数，此参数是客户端与服务器建立连接时所获得的返回值，也就是说这个值标识了这条TCP连接，确认了消息发送、接收的双方。</p>
</blockquote>
<p>__</p>
<pre><code># define RecvMsg(buf)                                    \
       ret = recv(newfd,buf,MAX_BUF_LEN,0);             \
       if(ret &gt; 0)                                      \
       &#123;                                                \
            printf(&quot;recv \&quot;%s\&quot; from %s:%d\n&quot;,          \
            buf,                                        \
            (char*)inet_ntoa(clientaddr.sin_addr),      \
            ntohs(clientaddr.sin_port));                \
       &#125;

#define SendMsg(buf)                                    \
        ret = send(newfd,buf,strlen(buf),0);            \
        if(ret &gt; 0)                                     \
        &#123;                                               \
            printf(&quot;send \&quot;hi\&quot; to %s:%d\n&quot;,            \
            (char*)inet_ntoa(clientaddr.sin_addr),      \
            ntohs(clientaddr.sin_port));                \
        &#125;
</code></pre>
<p>因此，在消息接收与发送中我们所使用到的<code>Socket API</code>共有两个：<strong>recv，send</strong> 。</p>
<h3 id="ServiceStop与ShutdownService方法"><a href="#ServiceStop与ShutdownService方法" class="headerlink" title="ServiceStop与ShutdownService方法"></a>ServiceStop与ShutdownService方法</h3><blockquote>
<p>这两个方法都用于终止连接，因此在代码中所调用的API也都是<code>close</code>。</p>
</blockquote>
<p>__</p>
<pre><code># define ShutdownService()                               \
        close(sockfd);
        
#define ServiceStop()                                   \
        close(newfd);
</code></pre>
<p>因此，在停止和关闭服务中我们所使用到的<code>Socket API</code>为：<strong>close</strong> 。</p>
<h2 id="2-2-客户端"><a href="#2-2-客户端" class="headerlink" title="2 |**2**客户端"></a><strong><em>2</em></strong> |**<em>2</em>**<strong>客户端</strong></h2><blockquote>
<p>同样的，在我们使用<code>hello</code>命令时，程序会调用Hello函数。</p>
</blockquote>
<p>__</p>
<pre><code>MenuConfig( &quot;hello&quot;, &quot;Hello TCP Client&quot;, Hello);
</code></pre>
<blockquote>
<p>在Hello函数中，我们又见到了消息发送和接收这两个方法的“身影”。</p>
</blockquote>
<p>__</p>
<pre><code>int Hello(int argc, char *argv[])
&#123;
    char szBuf[MAX_BUF_LEN] = &quot;\0&quot;;
    char szMsg[MAX_BUF_LEN] = &quot;hello\0&quot;;
    OpenRemoteService();
    SendMsg(szMsg);
    RecvMsg(szBuf);
    CloseRemoteService();
    return 0;
&#125;
</code></pre>
<p>其调用的方法内部实现与服务器端类似，这里我们给出结论：</p>
<p>客户端所使用到的Socket API有：<strong>socket， connect，send，recv，close</strong> 。</p>
<hr>
<h1 id="3-0-跟踪socket相关系统调用的内核处理函数"><a href="#3-0-跟踪socket相关系统调用的内核处理函数" class="headerlink" title="3 |**0**跟踪socket相关系统调用的内核处理函数"></a><strong><em>3</em></strong> |**<em>0</em>**<strong>跟踪socket相关系统调用的内核处理函数</strong></h1><h2 id="3-1-系统调用初始化"><a href="#3-1-系统调用初始化" class="headerlink" title="3 |**1**系统调用初始化"></a><strong><em>3</em></strong> |**<em>1</em>**<strong>系统调用初始化</strong></h2><p>系统调用初始化的过程：</p>
<blockquote>
<ul>
<li>在32位系统中：<code>start_kernel -&gt; trap_init -&gt; idt_setup_traps -&gt; 0x80</code></li>
<li>在64位系统中：<code>start_kernel -&gt; trap_init -&gt; cpu_init -&gt; syscall_init</code></li>
</ul>
</blockquote>
<p>为了验证这个初始化过程是否正确，我们使用上一次实验使用过的<code>gdb</code>进行调试。</p>
<p>由于我们上次实验构建的是32位的MenuOS系统，因此我们在<code>start_kernel</code>， <code>trap_init</code>以及<br><code>idt_setup_traps</code>这三个函数打上断点来完成验证。</p>
<blockquote>
<p>命令</p>
</blockquote>
<p>__</p>
<pre><code>sudo qemu -kernel ../linux -5.0.1/arch/x86/boot/bzImage -initrd ../rootfs.img -append  nokaslr -s -S #启动

# 新建终端
gdb #进入gdb命令行
file ../LinuxKernel/linux-5.0.1/vmlinux #在gdb界面中targe remote之前加载符号表
target remote:1234 # 建立gdb和gdbserver之间的连接
break start_kernel # 设置断点1
break trap_init # 设置断点2
break idt_setup_traps # 设置断点3
</code></pre>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220313106-160394591.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220313106-160394591.png"></a></p>
<p>接着，我们可以使用<code>continue</code>命令让<code>qemu</code>虚拟机继续运行起来。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220327771-703485049.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220327771-703485049.png"></a></p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220337160-1475253107.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220337160-1475253107.png"></a></p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220353159-565830860.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220353159-565830860.png"></a></p>
<p>为避免出现断点捕捉顺序与断点设置顺序相关的情况发生，这里我故意将后两个函数的断点设置顺序调换了一下。</p>
<p>而结果如我们之前分析的一样，我们捕获到了三个断点，并且这三个断点的捕捉顺序正是系统调用初始化过程所经历的顺序。</p>
<h2 id="3-2-socket相关的系统调用"><a href="#3-2-socket相关的系统调用" class="headerlink" title="3 |**2**socket相关的系统调用"></a><strong><em>3</em></strong> |**<em>2</em>**<strong>socket相关的系统调用</strong></h2><p>通过查阅<a target="_blank" rel="noopener" href="https://github.com/mengning/linux/blob/master/arch/x86/entry/syscalls/syscall_32.tbl">32位的系统调用表</a>，我们发现与<code>socket</code>相关的系统调用分为以下两种：</p>
<blockquote>
<ul>
<li>sys_socketcall</li>
</ul>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220422911-665150556.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220422911-665150556.png"></a></p>
<ul>
<li>socket api对应的单独系统调用</li>
</ul>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220442073-678615520.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220442073-678615520.png"></a></p>
</blockquote>
<p>通过查阅资料我们发现：</p>
<p><code>sys_socketcall()</code>是内核中为 socket 设置的<code>总入口 </code>，也就是说，在调用socket其他相关的<code>API</code>时都需要先调用sys_socketcall()。</p>
<p>而在前一章中的<code>实验代码分析</code>中，我们对于已经集成到<code>MenuOS</code>中<code>replyhi</code>和<code>hello</code>命令所对应的客户端、服务器端程序进行了分析，并且已经得出结论，即这两个命令在执行时所使用到的<code>Socket API</code>。分别为：</p>
<p>__</p>
<pre><code>服务器端：socket，bind，listen，accept，recv，send，close 客户端：socket，connect，send，recv，close
</code></pre>
<p>理清楚了<code>socket</code>相关的系统调用之后，下面我们同样通过<code>gdb</code>调试的方法来验证我们的分析是否正确。</p>
<blockquote>
<p>命令</p>
</blockquote>
<p>__</p>
<pre><code>break sys_socketcall # 设置断点
continue 
</code></pre>
<p>经过调试，我们捕捉到了该断点，并通过<code>list</code>命令找到了<code>sys_socketcall</code>系统调用所对应的内核处理函数<code>SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)</code>。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220038758-277367624.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220038758-277367624.png"></a></p>
<p>那么，下面简要的分析以下这个<a target="_blank" rel="noopener" href="https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/net/socket.c?h=v5.0.1">内核处理函数</a>。</p>
<p>__</p>
<pre><code>/* *	System call vectors.
 *
 *	Argument checking cleaned up. Saved 20% in size.
 *  This function doesn&#39;t need to set the kernel lock because
 *  it is set by the callees.
 */

SYSCALL_DEFINE2(socketcall, int, call, unsigned long __user *, args)
&#123;
    unsigned long a[AUDITSC_ARGS];
    unsigned long a0, a1;
    int err;
    unsigned int len;

    if (call &lt; 1 || call &gt; SYS_SENDMMSG)
        return -EINVAL;
    call = array_index_nospec(call, SYS_SENDMMSG + 1);

    len = nargs[call];
    if (len &gt; sizeof(a))
        return -EINVAL;

    /* copy_from_user should be SMP safe. */
    if (copy_from_user(a, args, len))
        return -EFAULT;

    err = audit_socketcall(nargs[call] / sizeof(unsigned long), a);
    if (err)
        return err;

    a0 = a[0];
    a1 = a[1];

    switch (call) &#123;
    case SYS_SOCKET:
        err = __sys_socket(a0, a1, a[2]);
        break;
    case SYS_BIND:
        err = __sys_bind(a0, (struct sockaddr __user *)a1, a[2]);
        break;
    case SYS_CONNECT:
        err = __sys_connect(a0, (struct sockaddr __user *)a1, a[2]);
        break;
    case SYS_LISTEN:
        err = __sys_listen(a0, a1);
        break;
    case SYS_ACCEPT:
        err = __sys_accept4(a0, (struct sockaddr __user *)a1,
                    (int __user *)a[2], 0);
        break;
    case SYS_GETSOCKNAME:
        err =
            __sys_getsockname(a0, (struct sockaddr __user *)a1,
                      (int __user *)a[2]);
        break;
    case SYS_GETPEERNAME:
        err =
            __sys_getpeername(a0, (struct sockaddr __user *)a1,
                      (int __user *)a[2]);
        break;
    case SYS_SOCKETPAIR:
        err = __sys_socketpair(a0, a1, a[2], (int __user *)a[3]);
        break;
    case SYS_SEND:
        err = __sys_sendto(a0, (void __user *)a1, a[2], a[3],
                   NULL, 0);
        break;
    case SYS_SENDTO:
        err = __sys_sendto(a0, (void __user *)a1, a[2], a[3],
                   (struct sockaddr __user *)a[4], a[5]);
        break;
    case SYS_RECV:
        err = __sys_recvfrom(a0, (void __user *)a1, a[2], a[3],
                     NULL, NULL);
        break;
    case SYS_RECVFROM:
        err = __sys_recvfrom(a0, (void __user *)a1, a[2], a[3],
                     (struct sockaddr __user *)a[4],
                     (int __user *)a[5]);
        break;
    case SYS_SHUTDOWN:
        err = __sys_shutdown(a0, a1);
        break;
    case SYS_SETSOCKOPT:
        err = __sys_setsockopt(a0, a1, a[2], (char __user *)a[3],
                       a[4]);
        break;
    case SYS_GETSOCKOPT:
        err =
            __sys_getsockopt(a0, a1, a[2], (char __user *)a[3],
                     (int __user *)a[4]);
        break;
    case SYS_SENDMSG:
        err = __sys_sendmsg(a0, (struct user_msghdr __user *)a1,
                    a[2], true);
        break;
    case SYS_SENDMMSG:
        err = __sys_sendmmsg(a0, (struct mmsghdr __user *)a1, a[2],
                     a[3], true);
        break;
    case SYS_RECVMSG:
        err = __sys_recvmsg(a0, (struct user_msghdr __user *)a1,
                    a[2], true);
        break;
    case SYS_RECVMMSG:
        if (IS_ENABLED(CONFIG_64BIT) || !IS_ENABLED(CONFIG_64BIT_TIME))
            err = __sys_recvmmsg(a0, (struct mmsghdr __user *)a1,
                         a[2], a[3],
                         (struct __kernel_timespec __user *)a[4],
                         NULL);
        else
            err = __sys_recvmmsg(a0, (struct mmsghdr __user *)a1,
                         a[2], a[3], NULL,
                         (struct old_timespec32 __user *)a[4]);
        break;
    case SYS_ACCEPT4:
        err = __sys_accept4(a0, (struct sockaddr __user *)a1,
                    (int __user *)a[2], a[3]);
        break;
    default:
        err = -EINVAL;
        break;
    &#125;
    return err;
&#125;
</code></pre>
<p>其核心代码在于<code>switch...case</code>中，可以看到在函数执行时它会按照参数<code>call</code>的值去调用不同的内核处理程序，如<code>__sys_socket</code>，<code>__sys_bind</code>等。至于call的值，<code>SYS_SOCKET</code>对应值为1，下面的<code>case</code>条件所对应的call值则以此递增。</p>
<p>这样，我们就很容易理解为什么说<code>sys_socketcall()</code>是内核中为 socket 设置的总入口。</p>
<p>接着，我们将<code>Socket API</code>所对应的各个内核处理函数打上断点。</p>
<blockquote>
<p>命令</p>
</blockquote>
<p>__</p>
<pre><code>break  __sys_socket
break  __sys_bind
break  __sys_listen
break  __sys_connect
break  __sys_accept4
break  __sys_recvmsg
break  __sys_sendmsg
break  __sys_recvfrom
break  __sys_sendto
break  __sys_shutdown
</code></pre>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220104626-1320211378.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220104626-1320211378.png"></a></p>
<p>然后，使用<code>continue</code>命令来康康我们是否能够捕获到这些断点。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220110932633-776686874.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220110932633-776686874.png"></a></p>
<p>可以发现在初始化的过程中我们共捕获到6次断点，分别是三次<code>sys_socketcall()</code>和三次<code>__sys_socket</code>。</p>
<p>都是三次，有这么巧合吗？另外，我们可以发现捕获顺序的规律总是先<code>sys_socketcall()</code>后捕获<code>__sys_socket</code>。</p>
<p>其实，我们之前已经提到过：<code>sys_socketcall()</code>是内核为socket设置的总入口，我们需要先进入到这个总入口，然后根据<code>call</code>值来进入不同的分支去调用对应的内核处理函数，这也就是为什么我们捕获断点时有上面的规律。</p>
<p>通过<code>qemu</code>虚拟机中的提示信息，该初始化的过程其实主要完成的是对网卡的配置与启动。</p>
<hr>
<p>继续。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220112841339-722490150.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220112841339-722490150.png"></a></p>
<p>在又一次捕获到<code>sys_socketcall()</code>且此时call值为1时，我们发现gdb调试界面停止，</p>
<p>这时，按照之前服务器端代码分析的思路，我们需要在<code>MenuOS</code>系统输入<code>replyhi</code>命令来创建socket。</p>
<p>在输入命令之后，gdb调试继续。并且再次捕获断点时可以看到我们捕获到了<code>__sys_socket</code>，即调用了该内核处理函数完成了我们服务器端socket的创建。</p>
<hr>
<p>继续捕获断点：</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220119325-1227384307.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220119325-1227384307.png"></a></p>
<p>与之前分析的一样，需要先进入<code>sys_socketcall()</code>这个总入口；</p>
<p>然后通过我们之前在源代码分析中看到的<strong>switch结构</strong> ，根据<code>call=2</code>的信息，可以知道它进入了<code>case SYS_BIND</code>这个分支；</p>
<p>继续使用<code>continue</code>命令可以看到，我们捕获到了<code>__sys_bind</code>这个断点，即确实按照分支的语句调用了所对应的内核处理函数，在这里，我们所调用的是socket的绑定函数。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220135410-1102664032.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191219220135410-1102664032.png"></a></p>
<hr>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220113820375-991637153.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220113820375-991637153.png"></a></p>
<p>这一次，我们捕获到<code>__sys_accept4</code>，按照服务器端代码的思路，我们知道此时服务器准备好接收客户端的请求。</p>
<p>在<code>qemu</code>虚拟机中输入<code>hello</code>命令，来创建客户端socket。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220114004900-39215539.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220114004900-39215539.png"></a></p>
<p>没错，在经历<code>sys_socketcall()</code>总入口之后我们再一次使用<code>__sys_socket</code>完成了客户端socket的创建。</p>
<hr>
<p>同理，我们捕获到<code>__sys_connect</code>断点，该内核处理函数完成了客户端与服务器端的连接。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220114310702-177767569.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220114310702-177767569.png"></a></p>
<p>接着，我们会捕捉到收发消息相关的一系列系统调用，而与此同时，在<code>qemu</code>虚拟机中，我们也能够看到服务器与客户端交互的情况。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220114430988-1788804597.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220114430988-1788804597.png"></a></p>
<p>之后捕获断点我们又会<strong>重新创建</strong> 一个客户端socket。</p>
<p>其原因在于<code>Replyhi</code>的实现中使用到了while循环，即服务器端不会主动关闭服务，而是不断等待新的客户端连接，然后与其进行交互。实现的是一个简单的多线程功能。</p>
<p><a href="./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220115302859-2131351471.png"><img src="/./Socket%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90_files/1314885-20191220115302859-2131351471.png"></a></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        
        
    </div>
    <a href="/2024/05/09/mdstorage/domain/network/Socket%E7%9A%84%E5%B0%81%E8%A3%85/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/09/mdstorage/domain/network/%E4%B8%BB%E6%9C%BA%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C/">
        <h2 class="post-title">主机路由转发qemu虚拟机网络.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/9
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>将 机器 B 的 虚拟机网络 192.168.122.1&#x2F;24 路由转发出去提供给 机器 A 进行访问</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul>
<li>保证 B 机器可以访问 A ，wlp2s0 网卡状态 UP</li>
<li>B 配置 sysctl 参数</li>
<li>B 配置 路由伪装</li>
<li>A 配置 路由规则</li>
</ul>
<h2 id="B-配置-sysctl-参数"><a href="#B-配置-sysctl-参数" class="headerlink" title="B 配置 sysctl 参数"></a>B 配置 sysctl 参数</h2><pre><code class="Bash">sudo sysctl -w net.ipv4.icmp_echo_ignore_all=0
sudo sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=0
</code></pre>
<h2 id="B-配置路由伪装"><a href="#B-配置路由伪装" class="headerlink" title="B 配置路由伪装"></a>B 配置路由伪装</h2><pre><code class="Bash"># virbr0 是 qemu 的虚拟网络的网桥
sudo iptables -t nat -A POSTROUTING -o virbr0 -j MASQUERADE
# 允许所有流量，通过清除 LIBVIRT_FWI 链中的规则：
sudo iptables -F LIBVIRT_FWI
sudo iptables -A LIBVIRT_FWI -j ACCEPT

</code></pre>
<h2 id="A-配置路由规则"><a href="#A-配置路由规则" class="headerlink" title="A 配置路由规则"></a>A 配置路由规则</h2><pre><code class="Bash"># enp6s0 目标网卡   169.254.98.174 是 B 的 ip 地址
sudo ip route add 192.168.122.0/24 dev enp6s0 via 169.254.98.174  proto static
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/network/" style="color: #00a596">network</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/linux/" style="color: #00bcd4">linux</a>
        </span>
        
    </div>
    <a href="/2024/05/09/mdstorage/domain/network/%E4%B8%BB%E6%9C%BA%E8%B7%AF%E7%94%B1%E8%BD%AC%E5%8F%91qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/08/mdstorage/domain/python/conda/">
        <h2 class="post-title">conda.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/8
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="第-1-步-–-准备-Anaconda-安装程序"><a href="#第-1-步-–-准备-Anaconda-安装程序" class="headerlink" title="第 1 步 – 准备 Anaconda 安装程序"></a>第 1 步 – 准备 Anaconda 安装程序</h2><p>使用 cd 命令，进入<code>/tmp</code>目录：</p>
<pre><code>cd /tmp 
</code></pre>
<p>接下来，使用 curl 命令从官网下载 Anaconda 安装程序脚本。访问 Anaconda<br>安装程序脚本<a target="_blank" rel="noopener" href="https://repo.anaconda.com/archive/">下载页面</a>，以检查最新版本。执行命令下载脚本：</p>
<pre><code># 示例
curl --output anaconda.sh https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh 
</code></pre>
<p>要检查脚本 SHA-256 校验和：</p>
<pre><code>sha256sum anconda.sh 


Output：
25e3ebae8905450ddac0f5c93f89c467 anaconda.sh
</code></pre>
<p>检查哈希码是否与下载页面上显示的代码匹配。</p>
<h2 id="第-2-步-–-在-Debian-11-上安装-Anaconda"><a href="#第-2-步-–-在-Debian-11-上安装-Anaconda" class="headerlink" title="第 2 步 – 在 Debian 11 上安装 Anaconda"></a>第 2 步 – 在 Debian 11 上安装 Anaconda</h2><p>执行 Anaconda 安装程序脚本，如下所示：</p>
<pre><code>bash anaconda.sh 
</code></pre>
<p>按照向导说明完成 Anaconda 安装过程。</p>
<p>Anaconda 已成功安装在 Debian 上。使用以下命令激活 Anaconda 环境：</p>
<pre><code>source ~/.bashrc 
</code></pre>
<p>为了验证安装情况，我们打开 conda 列表。</p>
<pre><code>conda list 


Output:

# packages in environment at /home/tecadmin/anaconda3:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py38_0
_libgcc_mutex             0.1                        main
alabaster                 0.7.12             pyhd3eb1b0_0
anaconda                  2021.05                  py38_0
anaconda-client           1.7.2                    py38_0
anaconda-navigator        2.0.3                    py38_0
anaconda-project          0.9.1              pyhd3eb1b0_1
anyio                     2.2.0            py38h06a4308_1
appdirs                   1.4.4                      py_0
</code></pre>
<h2 id="如何更新-Anaconda"><a href="#如何更新-Anaconda" class="headerlink" title="如何更新 Anaconda"></a>如何更新 Anaconda</h2><p>可以使用 conda 二进制文件更新 Anaconda 软件包。要升级系统上的 Anaconda，请执行下面的命令：</p>
<pre><code>conda update --all 
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/conda/" style="color: #03a9f4">conda</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Anaconda/" style="color: #ffa2c4">Anaconda</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/python/" style="color: #ff7d73">python</a>
        </span>
        
    </div>
    <a href="/2024/05/08/mdstorage/domain/python/conda/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/07/mdstorage/domain/linux/%E6%9C%89%E7%BA%BF%E8%BF%9E%E6%8E%A5%E4%B8%A4%E5%8F%B0ubuntu/">
        <h2 class="post-title">有线连接两台ubuntu.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/7
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>通过网线连接两台物理机使相连</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul>
<li>网线连接</li>
<li>设置机器A</li>
<li>设置机器B</li>
</ul>
<h2 id="机器设置"><a href="#机器设置" class="headerlink" title="机器设置"></a>机器设置</h2><ul>
<li>设置 -&gt; 网络 -&gt; 选择网卡 mac -&gt; ipv4 选择手动，不需要设置网关</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/network/" style="color: #00a596">network</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/ubuntu/" style="color: #00bcd4">ubuntu</a>
        </span>
        
    </div>
    <a href="/2024/05/07/mdstorage/domain/linux/%E6%9C%89%E7%BA%BF%E8%BF%9E%E6%8E%A5%E4%B8%A4%E5%8F%B0ubuntu/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2024/05/07/mdstorage/domain/network/%E7%BD%91%E7%BB%9C%E6%89%AB%E6%8F%8F%E5%B7%A5%E5%85%B7/">
        <h2 class="post-title">网络扫描工具.md</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/domain/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                domain
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/7
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><h3 id="fping"><a href="#fping" class="headerlink" title="fping"></a>fping</h3><pre><code class="Bash"># apt install fping
# 扫描局域网 192.168.1.1/24 存活的主机
fping -a -g 192.168.1.1/24
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/network/" style="color: #00bcd4">network</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/scan/" style="color: #00a596">scan</a>
        </span>
        
    </div>
    <a href="/2024/05/07/mdstorage/domain/network/%E7%BD%91%E7%BB%9C%E6%89%AB%E6%8F%8F%E5%B7%A5%E5%85%B7/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <a class="page-num" href="/page/8/">
        <i class="fa-solid fa-caret-left fa-fw"></i>
    </a>
    <a class="page-num" href="/">1</a>
    <span class="page-omit">...</span>
    
    <span class="current">9</span>
    
    <a class="page-num" href="/page/10">
        10
    </a>
    
    
    <a class="page-num" href="/page/11">
        11
    </a>
    
    
    <span class="page-omit">...</span>
    <a class="page-num" href="/page/28">28</a>
    
    
    <a class="page-num" href="/page/10/">
        <i class="fa-solid fa-caret-right fa-fw"></i>
    </a>
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/avatar.jpg" alt="avatar" />
        </div>
        <div class="name">xiaoy</div>
        <div class="description">
            <p>xiaoy<br>…</p>

        </div>
        
        
        <div class="friend-links">
            
            <div class="friend-link">
                <a href="https://abrance.github.io">Argvchs</a>
            </div>
            
        </div>
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2024 - 2024 Xiaoy
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;xiaoy
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
</body>
</html>
